# Comparing `tmp/vampire_analysis-0.1.0.dev9-py3-none-any.whl.zip` & `tmp/vampire_analysis-0.2.0.dev1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,29 +1,29 @@
-Zip file size: 53895 bytes, number of entries: 27
+Zip file size: 54184 bytes, number of entries: 27
 -rw-r--r--  2.0 unx      239 b- defN 22-Mar-19 06:03 vampire/__init__.py
--rw-r--r--  2.0 unx    12406 b- defN 22-Aug-18 23:58 vampire/amath.py
--rw-r--r--  2.0 unx    10401 b- defN 22-Mar-21 06:37 vampire/analysis.py
--rw-r--r--  2.0 unx     3730 b- defN 22-Mar-11 01:27 vampire/coloring.py
--rw-r--r--  2.0 unx     9958 b- defN 22-Aug-18 21:58 vampire/extraction.py
--rw-r--r--  2.0 unx     9959 b- defN 22-Aug-18 23:12 vampire/model.py
--rw-r--r--  2.0 unx    10768 b- defN 22-Aug-18 22:45 vampire/pilot_draw.py
--rw-r--r--  2.0 unx    14901 b- defN 22-Aug-18 22:34 vampire/plot.py
--rw-r--r--  2.0 unx    13422 b- defN 22-Aug-18 22:24 vampire/processing.py
--rw-r--r--  2.0 unx    22286 b- defN 22-Aug-18 23:16 vampire/quickstart.py
--rw-r--r--  2.0 unx     3370 b- defN 22-Mar-20 00:44 vampire/util.py
+-rw-r--r--  2.0 unx    12406 b- defN 23-May-06 22:09 vampire/amath.py
+-rw-r--r--  2.0 unx    10286 b- defN 23-May-06 07:14 vampire/analysis.py
+-rw-r--r--  2.0 unx     3676 b- defN 23-May-06 07:14 vampire/coloring.py
+-rw-r--r--  2.0 unx     9990 b- defN 22-Oct-26 17:33 vampire/extraction.py
+-rw-r--r--  2.0 unx    10161 b- defN 23-May-07 20:19 vampire/model.py
+-rw-r--r--  2.0 unx    10762 b- defN 23-May-06 07:17 vampire/pilot_draw.py
+-rw-r--r--  2.0 unx    14888 b- defN 23-May-06 07:14 vampire/plot.py
+-rw-r--r--  2.0 unx    13427 b- defN 23-May-07 20:26 vampire/processing.py
+-rw-r--r--  2.0 unx    20472 b- defN 23-May-07 20:09 vampire/quickstart.py
+-rw-r--r--  2.0 unx     3262 b- defN 23-May-06 07:14 vampire/util.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Mar-08 17:39 vampire/tests/__init__.py
 -rw-r--r--  2.0 unx     3333 b- defN 22-Mar-08 17:39 vampire/tests/test_amath.py
--rw-r--r--  2.0 unx     6974 b- defN 22-Mar-21 04:58 vampire/tests/test_analysis.py
+-rw-r--r--  2.0 unx     6970 b- defN 23-May-06 07:14 vampire/tests/test_analysis.py
 -rw-r--r--  2.0 unx     2181 b- defN 22-Mar-08 18:47 vampire/tests/test_coloring.py
 -rw-r--r--  2.0 unx     9409 b- defN 22-Mar-18 23:55 vampire/tests/test_extraction.py
--rw-r--r--  2.0 unx     3446 b- defN 22-Mar-21 04:13 vampire/tests/test_model.py
--rw-r--r--  2.0 unx    16471 b- defN 22-Mar-20 01:24 vampire/tests/test_plot.py
+-rw-r--r--  2.0 unx     3436 b- defN 23-May-06 07:17 vampire/tests/test_model.py
+-rw-r--r--  2.0 unx    16505 b- defN 23-May-06 07:28 vampire/tests/test_plot.py
 -rw-r--r--  2.0 unx     4997 b- defN 22-Mar-08 18:31 vampire/tests/test_processing.py
--rw-r--r--  2.0 unx    10607 b- defN 22-Mar-21 04:39 vampire/tests/test_quickstart.py
+-rw-r--r--  2.0 unx    10447 b- defN 23-May-07 19:49 vampire/tests/test_quickstart.py
 -rw-r--r--  2.0 unx    12395 b- defN 22-Mar-21 04:17 vampire/tests/test_util.py
 -rw-r--r--  2.0 unx      663 b- defN 22-Mar-08 19:05 vampire/tests/testing.py
--rwxr-xr-x  2.0 unx    35821 b- defN 22-Aug-19 01:04 vampire_analysis-0.1.0.dev9.dist-info/LICENSE
--rw-r--r--  2.0 unx     3876 b- defN 22-Aug-19 01:04 vampire_analysis-0.1.0.dev9.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 22-Aug-19 01:04 vampire_analysis-0.1.0.dev9.dist-info/WHEEL
--rw-r--r--  2.0 unx        8 b- defN 22-Aug-19 01:04 vampire_analysis-0.1.0.dev9.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     2228 b- defN 22-Aug-19 01:04 vampire_analysis-0.1.0.dev9.dist-info/RECORD
-27 files, 223941 bytes uncompressed, 50335 bytes compressed:  77.5%
+-rwxr-xr-x  2.0 unx    35821 b- defN 23-May-07 21:11 vampire_analysis-0.2.0.dev1.dist-info/LICENSE
+-rw-r--r--  2.0 unx     3884 b- defN 23-May-07 21:11 vampire_analysis-0.2.0.dev1.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-May-07 21:11 vampire_analysis-0.2.0.dev1.dist-info/WHEEL
+-rw-r--r--  2.0 unx        8 b- defN 23-May-07 21:11 vampire_analysis-0.2.0.dev1.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     2229 b- defN 23-May-07 21:11 vampire_analysis-0.2.0.dev1.dist-info/RECORD
+27 files, 221939 bytes uncompressed, 50624 bytes compressed:  77.2%
```

## zipnote {}

```diff
@@ -60,23 +60,23 @@
 
 Filename: vampire/tests/test_util.py
 Comment: 
 
 Filename: vampire/tests/testing.py
 Comment: 
 
-Filename: vampire_analysis-0.1.0.dev9.dist-info/LICENSE
+Filename: vampire_analysis-0.2.0.dev1.dist-info/LICENSE
 Comment: 
 
-Filename: vampire_analysis-0.1.0.dev9.dist-info/METADATA
+Filename: vampire_analysis-0.2.0.dev1.dist-info/METADATA
 Comment: 
 
-Filename: vampire_analysis-0.1.0.dev9.dist-info/WHEEL
+Filename: vampire_analysis-0.2.0.dev1.dist-info/WHEEL
 Comment: 
 
-Filename: vampire_analysis-0.1.0.dev9.dist-info/top_level.txt
+Filename: vampire_analysis-0.2.0.dev1.dist-info/top_level.txt
 Comment: 
 
-Filename: vampire_analysis-0.1.0.dev9.dist-info/RECORD
+Filename: vampire_analysis-0.2.0.dev1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## vampire/analysis.py

```diff
@@ -12,59 +12,59 @@
 
 
 def get_cum_explained_variance_ratio(explained_variance_ratio):
     cum_explained_variance_ratio = np.cumsum(explained_variance_ratio)
     return cum_explained_variance_ratio
 
 
-def get_optimal_num_pc(cum_explained_variance_ratio, ratio=0.95):
-    num_pc = np.sum(cum_explained_variance_ratio < ratio) + 1  # add one to exceed the ratio
-    if num_pc > len(cum_explained_variance_ratio):  # prevent index out of range
-        num_pc = len(cum_explained_variance_ratio)
-    return num_pc
+def get_optimal_n_pcs(cum_explained_variance_ratio, ratio=0.95):
+    n_pcs = np.sum(cum_explained_variance_ratio < ratio) + 1  # add one to exceed the ratio
+    if n_pcs > len(cum_explained_variance_ratio):  # prevent index out of range
+        n_pcs = len(cum_explained_variance_ratio)
+    return n_pcs
 
 
 def pca_transform_contours(contours, mean_contour, principal_directions):
     """
     Transform contour coordinates to principal directions in the PC space.
 
     Parameters
     ----------
     contours : ndarray
-        Object contours, with shape (num_contour, 2*num_points).
+        Object contours, with shape (n_contours, 2*n_points).
     mean_contour : ndarray
         Mean contour used to mean-center object contours.
     principal_directions : ndarray
         Loadings, weights, principal directions, principal axes,
         eigenvector of covariance matrix of mean-subtracted contours,
-        with shape (2*num_points, 2*num_points).
+        with shape (2*n_points, 2*n_points).
 
     Returns
     -------
     principal_components : ndarray
         PC score, principal components, coordinates of mean-subtracted contours
-        in their principal directions, with shape (num_contours, 2*num_points).
+        in their principal directions, with shape (n_contours, 2*n_points).
 
     """
     mean_centered_contours = contours - mean_contour
     principal_components = mean_centered_contours @ principal_directions
     return principal_components
 
 
-def cluster_contours(pc, num_clusters=5, num_pc=20, random_state=None):
+def cluster_contours(pc, n_clusters=5, n_pcs=20, random_state=None):
     """
     K-means clustering of contour principal components.
 
     Parameters
     ----------
     pc : ndarray
         Principal components of contours.
-    num_clusters : int, optional
+    n_clusters : int, optional
         Number of clusters.
-    num_pc : int, optional
+    n_pcs : int, optional
         Number of principal components used for approximation.
     random_state : None or int, optional
         Random state for K-means clustering.
 
     Returns
     -------
     cluster_id_df : DataFrame
@@ -75,59 +75,63 @@
         Sum of squared distances of samples to their closest cluster center.
 
     See Also
     --------
     sklearn.cluster.KMeans : Implementation of K-means clustering.
 
     """
-    pc_truncated = pc[:, :num_pc]
+    pc_truncated = pc[:, :n_pcs]
     pc_truncated_normalized = preprocessing.normalize(pc_truncated)
 
     # k-means clustering of normalized principal coordinates
-    k_means = KMeans(n_clusters=num_clusters,
-                     random_state=random_state,
-                     init='k-means++',
-                     n_init=3,
-                     max_iter=300).fit(pc_truncated_normalized)
+    k_means = KMeans(
+        n_clusters=n_clusters,
+        random_state=random_state,
+        init='k-means++',
+        n_init=3,
+        max_iter=300
+    ).fit(pc_truncated_normalized)
     centroids = k_means.cluster_centers_
     inertia = k_means.inertia_
     distance = spatial.distance.cdist(pc_truncated_normalized, centroids)
     cluster_id = np.argmin(distance, axis=1)
     min_distance = np.min(distance, axis=1)
 
     # tag each object with cluster id
-    cluster_id_df = pd.DataFrame({'cluster_id': cluster_id,
-                                  'distance_to_centroid': min_distance})
+    cluster_id_df = pd.DataFrame({
+        'cluster_id': cluster_id,
+        'distance_to_centroid': min_distance
+    })
     return cluster_id_df, centroids, inertia
 
 
-def assign_clusters_id(pc, contours, centroids, num_pc=20):
+def assign_clusters_id(pc, contours, centroids, n_pcs=20):
     """
     Assign the contours with id of the closest centroid.
 
     Parameters
     ----------
     pc : ndarray
         Principal components of contours.
     contours : ndarray
-        Object contours, with shape (num_contour, 2*num_points).
+        Object contours, with shape (n_contours, 2*n_points).
     centroids : ndarray
         Coordinates of cluster centers of K-means clusters.
-    num_pc : int, optional
+    n_pcs : int, optional
         Number of principal components used for approximation.
 
     Returns
     -------
     contours_df : DataFrame
         DataFrame of objects' contour coordinates, cluster id,
         and min distance from centroid.
 
     """
-    # find closest centroid and get cluster id
-    pc_truncated = pc[:, :num_pc]
+    # find the closest centroid and get cluster id
+    pc_truncated = pc[:, :n_pcs]
     # Original VAMPIRE GUI software did not normalize when
     # assigning clusters. However, it is logical to keep the
     # input of clustering and classifying consistent, so that
     # the same data used in clustering and assign cluster give
     # the same result.
     pc_truncated = preprocessing.normalize(pc_truncated)
 
@@ -135,26 +139,27 @@
     cluster_id = np.argmin(distance, axis=1)
     min_distance = np.min(distance, axis=1)
 
     # tag each object with cluster id
     normalized_contours = {'normalized_contour': list(contours)}
     contours_df = pd.DataFrame(normalized_contours)
     contours_df['cluster_id'] = cluster_id
+    contours_df['plot_cluster_id'] = cluster_id + 1  # avoid zero-indexing for plotting
     contours_df['distance_to_centroid'] = min_distance
     return contours_df
 
 
 def get_labeled_contours_df(contours, cluster_id_df):
     """
     Return contour coordinates, cluster id, and distance to centroid.
 
     Parameters
     ----------
     contours : ndarray
-        Object contours, with shape (num_contour, 2*num_points).
+        Object contours, with shape (n_contours, 2*n_points).
     cluster_id_df : DataFrame
         DataFrame of objects' cluster id and min distance to centroid.
 
     Returns
     -------
     labeled_contours_df : DaraFrame
         DataFrame of contour coordinates, cluster id, and min
@@ -213,19 +218,21 @@
     scipy.cluster.hierarchy.linkage
     scipy.cluster.hierarchy.dendrogram
 
     """
     mean_cluster_contours = get_mean_cluster_contours(labeled_contours_df)
     pair_distance = spatial.distance.pdist(mean_cluster_contours, 'euclidean')
     linkage_matrix = cluster.hierarchy.linkage(pair_distance, method='complete')
-    branches = cluster.hierarchy.dendrogram(linkage_matrix,
-                                            p=0,
-                                            truncate_mode='lastp',
-                                            orientation='bottom',
-                                            above_threshold_color='k')
+    branches = cluster.hierarchy.dendrogram(
+        linkage_matrix,
+        p=0,
+        truncate_mode='lastp',
+        orientation='bottom',
+        above_threshold_color='k'
+    )
     plt.close()
     return pair_distance, linkage_matrix, branches
 
 
 def get_cluster_order(branches):
     """
     Get the cluster id of contours in order of dendrogram.
```

## vampire/coloring.py

```diff
@@ -96,31 +96,33 @@
     # avoid modifying img in outer scope with inplace operations
     img = np.copy(img)
 
     # assign each cluster a label that's normalized for cmap
     cluster_ids = np.unique(img)
     cluster_ids = np.delete(cluster_ids,
                             np.where(cluster_ids == background))
-    num_clusters = len(cluster_ids)
-    replaced_labels = np.linspace(0.1, 0.9, num_clusters)
+    n_clusters = len(cluster_ids)
+    replaced_labels = np.linspace(0.1, 0.9, n_clusters)
     for i, replaced_label in enumerate(replaced_labels):
         img[img == cluster_ids[i]] = replaced_label
 
     # make background "bad" so it displays background_color
     img[img == background] = np.nan
 
     # plot cluster-labeled img
     fig, ax = plt.subplots(figsize=(5, 5))
     ax.imshow(img, cmap=cmap, vmax=1, vmin=0)
-    ax.tick_params(axis='both',
-                   which='both',
-                   bottom=False,
-                   top=False,
-                   left=False,
-                   labelbottom=False,
-                   labelleft=False)
+    ax.tick_params(
+        axis='both',
+        which='both',
+        bottom=False,
+        top=False,
+        left=False,
+        labelbottom=False,
+        labelleft=False
+    )
     plt.tight_layout(pad=0)
 
     # colors used for labeling
     colors = cmap(replaced_labels)
 
     return fig, ax, colors
```

## vampire/extraction.py

```diff
@@ -107,17 +107,19 @@
 
     Returns
     -------
     contour : ndarray
         x and y coordinates of n contour sample points, with shape (2, n)
 
     """
-    contour = cv2.findContours(object_img.astype('uint8'),
-                               cv2.RETR_TREE,
-                               cv2.CHAIN_APPROX_NONE)[0][0]
+    contour = cv2.findContours(
+        object_img.astype('uint8'),
+        cv2.RETR_TREE,
+        cv2.CHAIN_APPROX_NONE
+    )[0][0]
     contour = contour.reshape(-1, 2).T
     contour = np.flip(contour, axis=1)
     if contour.size <= 6:  # contour has <= 3 points, could not be sampled
         return np.nan
     return contour
 
 
@@ -137,41 +139,51 @@
     Returns
     -------
     properties_df : DataFrame
         Dataframe of object properties.
 
     """
     # get properties of objects
-    properties = ('label',
-                  'centroid',
-                  'area',
-                  'bbox_area',
-                  'convex_area',
-                  'filled_area',
-                  'perimeter',
-                  'equivalent_diameter',
-                  'major_axis_length',
-                  'minor_axis_length',
-                  'orientation',
-                  'euler_number',
-                  'eccentricity',
-                  'solidity',
-                  'extent')
-    properties_dict = regionprops_table(img,
-                                        properties=properties,
-                                        extra_properties=(extract_contour_from_object,))
+    properties = (
+        'label',
+        'centroid',
+        'area',
+        'bbox_area',
+        'convex_area',
+        'filled_area',
+        'perimeter',
+        'equivalent_diameter',
+        'major_axis_length',
+        'minor_axis_length',
+        'orientation',
+        'euler_number',
+        'eccentricity',
+        'solidity',
+        'extent'
+    )
+    properties_dict = regionprops_table(
+        img,
+        properties=properties,
+        extra_properties=(extract_contour_from_object,)
+    )
     properties_df = pd.DataFrame(properties_dict)
-    properties_df.rename(columns={'centroid-0': 'centroid-y',
-                                  'centroid-1': 'centroid-x',
-                                  'extract_contour_from_object': 'raw_contour'},
-                         inplace=True)
+    properties_df.rename(
+        columns={
+            'centroid-0': 'centroid-y',
+            'centroid-1': 'centroid-x',
+            'extract_contour_from_object': 'raw_contour'
+        },
+        inplace=True
+    )
     # additional properties
     properties_df['circularity'] = 4 * np.pi * properties_df['area'] / properties_df['perimeter'] ** 2
-    properties_df['aspect_ratio'] = np.nan_to_num(np.divide(properties_df['major_axis_length'],
-                                                            properties_df['minor_axis_length']))
+    properties_df['aspect_ratio'] = np.nan_to_num(np.divide(
+        properties_df['major_axis_length'],
+        properties_df['minor_axis_length']
+    ))
     # discard contours with <= 3 points that cannot be sampled
     properties_df = properties_df[pd.notna(properties_df['raw_contour'])]
     # label each object
     if img_id is not None:
         properties_df.insert(0, 'image_id', img_id)
     if filename is not None:
         properties_df.insert(0, 'filename', filename)
@@ -199,20 +211,24 @@
         raise ValueError('Length of img_set and filenames does not match.')
     properties_from_img_set = []
     for img_i, img in enumerate(img_set):
         if filenames is not None:
             filename = filenames[img_i]
         else:
             filename = None
-        properties_from_img = extract_properties_from_img(img,
-                                                          filename=filename,
-                                                          img_id=img_i)
+        properties_from_img = extract_properties_from_img(
+            img,
+            filename=filename,
+            img_id=img_i
+        )
         properties_from_img_set.append(properties_from_img)
-    properties_from_img_set_df = pd.concat(properties_from_img_set,
-                                           ignore_index=True)
+    properties_from_img_set_df = pd.concat(
+        properties_from_img_set,
+        ignore_index=True
+    )
     return properties_from_img_set_df
 
 
 def read_properties(img_set_path, filter_info):
     """
     Read object properties from existing property ``pickle`` file.
 
@@ -231,49 +247,56 @@
 
     """
     properties_pickle_path = util.get_properties_pickle_path(img_set_path, filter_info)
     properties_df = util.read_pickle(properties_pickle_path)
     return properties_df
 
 
-def write_properties(properties_df, img_set_path, filter_info):
+def write_properties(properties_df, img_set_path, filter_info, write_contour=False):
     """
     Writes contour coordinates and properties to given paths.
 
     Parameters
     ----------
     properties_df : DataFrame
         DataFrame of object properties.
     img_set_path : str
         Path to the directory of images to be analyzed.
     filter_info : ndarray
         Regex filter(s) of image filenames to be analyzed.
         Empty if no filter needed.
+    write_contour : bool, optional
+        Whether write and save raw contour coordinates.
 
     """
     properties_csv_path = util.get_properties_csv_path(img_set_path, filter_info)
     properties_pickle_path = util.get_properties_pickle_path(img_set_path, filter_info)
-    properties_df.drop('raw_contour', axis=1).to_csv(properties_csv_path, index=False)
+    if write_contour:
+        properties_df.to_csv(properties_csv_path, index=False)
+    else:
+        properties_df.drop('raw_contour', axis=1).to_csv(properties_csv_path, index=False)
     util.write_pickle(properties_pickle_path, properties_df)
     return
 
 
-def extract_properties(img_set_path, filter_info=None, write=True):
+def extract_properties(img_set_path, filter_info=None, write=True, write_contour=False):
     """
     Extracts object properties from image set path.
 
     Parameters
     ----------
     img_set_path : str
         Path to the directory of images to be analyzed.
     filter_info : ndarray, optional
         Regex filter(s) of image filenames to be analyzed.
         Empty if no filter needed.
-    write : bool
+    write : bool, optional
         Write properties into ``csv`` and ``pickle`` file.
+    write_contour : bool, optional
+        Whether write and save raw contour coordinates.
 
     Returns
     -------
     properties_df : Dataframe
         Dataframe of object properties.
 
     """
@@ -288,16 +311,18 @@
     elif full_set_exist:
         # extract specific set info from full set
         filenames = get_filtered_filenames(img_set_path, filter_info)
         full_properties_df = read_properties(img_set_path, empty_filter)
         filename_filter = np.isin(full_properties_df['filename'], filenames)
         properties_df = full_properties_df[filename_filter].reset_index(drop=True)
         if write:
-            write_properties(properties_df, img_set_path, filter_info)
+            write_properties(properties_df, img_set_path, filter_info, write_contour=write_contour)
     else:
         filenames = get_filtered_filenames(img_set_path, filter_info)
         img_set = get_img_set(img_set_path, filenames)
-        properties_df = extract_properties_from_img_set(img_set,
-                                                        filenames=filenames)
+        properties_df = extract_properties_from_img_set(
+            img_set,
+            filenames=filenames
+        )
         if write:
-            write_properties(properties_df, img_set_path, filter_info)
+            write_properties(properties_df, img_set_path, filter_info, write_contour=write_contour)
     return properties_df
```

## vampire/model.py

```diff
@@ -7,22 +7,22 @@
     """
     Visually Aided Morpho-Phenotyping Image Recognition (VAMPIRE) model.
 
     Attributes
     ----------
     model_name : str
         Name of the VAMPIRE model.
-    num_points : int
+    n_points : int
         Number of sample points in a contour.
-    num_coord : int
+    n_coords : int
         Number of coordinates in a contour.
-        num_coord = 2*num_points
-    num_clusters : int
+        n_coords = 2*n_points
+    n_clusters : int
         Number of clusters for K-Means clustering.
-    num_pc : int
+    n_pcs : int
         Number of principal components kept for analysis.
     random_state : int
         Random state of random processes.
     mean_registered_contour : ndarray
         Mean registered contour.
     mean_aligned_contour : ndarray
         Mean aligned contour.
@@ -42,44 +42,46 @@
         Pair distance between each cluster
     linkage_matrix : ndarray
         Linkage matrix for cluster dendrogram.
     branches : dict
         A dictionary of data structures computed to render the dendrogram.
 
     """
-    def __init__(self,
-                 model_name,
-                 num_points=50,
-                 num_clusters=5,
-                 num_pc=None,
-                 random_state=None):
+    def __init__(
+        self,
+        model_name,
+        n_points=50,
+        n_clusters=5,
+        n_pcs=None,
+        random_state=None
+    ):
         """
         Initialize VAMPIRE model wih hyperparameters.
 
         Parameters
         ----------
         model_name : str
             Name of the VAMPIRE model.
-        num_points : int, optional
+        n_points : int, optional
             Number of sample points for contour.
-        num_clusters : int, optional
+        n_clusters : int, optional
             Number of cluster for K-Means clustering.
-        num_pc : int, optional
+        n_pcs : int, optional
             Number of principal components to keep in analysis.
         random_state : int or None, optional
             Determines random number generation for K-Means clustering.
             Use an int to make the randomness deterministic.
 
         """
         # model hyperparameters
         self.model_name = model_name
-        self.num_points = num_points
-        self.num_coord = num_points * 2
-        self.num_clusters = num_clusters
-        self.num_pc = num_pc
+        self.n_points = n_points
+        self.n_coords = n_points * 2
+        self.n_clusters = n_clusters
+        self.n_pcs = n_pcs
         if random_state is not None:
             self.random_state = random_state
         else:
             self.random_state = np.random.default_rng().integers(100000)
         # contour info
         self.mean_registered_contour = None
         self.mean_aligned_contour = None
@@ -96,75 +98,74 @@
         self.inertia = None
         self.mean_cluster_contours = None
         # hierarchical clustering info
         self.pair_distance = None
         self.linkage_matrix = None
         self.branches = None
 
-    def build(self, properties_df):
+    def fit(self, properties_df):
         """
-        Builds VAMPIRE model from dataset.
+        Fit VAMPIRE model to dataset.
 
         Samples, registers, and aligns contour coordinates.
         PCA contour coordinates, then K-Means analysis.
         Hierarchical clustering determines cluster order.
 
         Parameters
         ----------
         properties_df : DataFrame
             DataFrame containing contour properties and raw contours.
+            Generated by `extraction.extract_properties`.
 
         Returns
         -------
         self : vampire.model.Vampire
-            Built VAMPIRE model.
+            VAMPIRE model.
 
         """
         # process contours
         contours = list(properties_df['raw_contour'])
-        contours = processing.sample_contours(contours, num_points=self.num_points)
+        contours = processing.sample_contours(contours, n_points=self.n_points)
         contours = processing.register_contours(contours)
         self.mean_registered_contour = processing.get_mean_registered_contour(contours)
         self.contours = processing.align_contours(contours, self.mean_registered_contour)
         self.mean_aligned_contour = processing.get_mean_aligned_contour(self.contours)
 
         # pca contours
-        (self.principal_directions,
-            principal_components,
-            self.explained_variance) = amath.pca(self.contours, 'eig')
+        self.principal_directions, principal_components, self.explained_variance = amath.pca(self.contours, 'eig')
         self.explained_variance_ratio = analysis.get_explained_variance_ratio(self.explained_variance)
         self.cum_explained_variance_ratio = analysis.get_cum_explained_variance_ratio(self.explained_variance_ratio)
-        if self.num_pc is None:
-            self.num_pc = analysis.get_optimal_num_pc(self.cum_explained_variance_ratio)
+        if self.n_pcs is None:
+            self.n_pcs = analysis.get_optimal_n_pcs(self.cum_explained_variance_ratio)
 
         # cluster contours
-        (self.cluster_id_df,
-            centroids,
-            self.inertia) = analysis.cluster_contours(principal_components,
-                                                      num_clusters=self.num_clusters,
-                                                      num_pc=self.num_pc,
-                                                      random_state=self.random_state)
+        self.cluster_id_df, centroids, self.inertia = analysis.cluster_contours(
+            principal_components,
+            n_clusters=self.n_clusters,
+            n_pcs=self.n_pcs,
+            random_state=self.random_state
+        )
         self.labeled_contours_df = analysis.get_labeled_contours_df(self.contours, self.cluster_id_df)
-        (self.pair_distance,
-            self.linkage_matrix,
-            self.branches) = analysis.hierarchical_cluster_contour(self.labeled_contours_df)
+        self.pair_distance, self.linkage_matrix, self.branches = analysis.hierarchical_cluster_contour(
+            self.labeled_contours_df
+        )
 
         # reorder clusters and centroid according to dendrogram
         # to be consistent with dendrogram visualization
         object_index = analysis.get_cluster_order(self.branches)
         cluster_id_sorted = analysis.reorder_clusters(self.cluster_id_df['cluster_id'], object_index)
         self.cluster_id_df['cluster_id'] = cluster_id_sorted
         self.labeled_contours_df['cluster_id'] = cluster_id_sorted
         self.mean_cluster_contours = analysis.get_mean_cluster_contours(self.labeled_contours_df)
         self.centroids = analysis.reorder_centroids(centroids, object_index)
         return self
 
-    def apply(self, properties_df):
+    def transform(self, properties_df):
         """
-        Applies built VAMPIRE model on dataset.
+        Transforms dataset using the VAMPIRE model.
 
         Parameters
         ----------
         properties_df : DataFrame
             DataFrame containing contour properties and raw contours.
 
         Returns
@@ -172,29 +173,55 @@
         properties_df : DataFrame
             DataFrame containing contour properties, raw contours,
             normalized contours, cluster id, and min distance from
             centroid.
 
         """
         contours = list(properties_df['raw_contour'])
-        contours = processing.sample_contours(contours,
-                                              num_points=self.num_points)
+        contours = processing.sample_contours(
+            contours,
+            n_points=self.n_points
+        )
         contours = processing.register_contours(contours)
         contours = processing.align_contours(contours, self.mean_registered_contour)
 
-        principal_components = analysis.pca_transform_contours(contours,
-                                                               self.mean_aligned_contour,
-                                                               self.principal_directions)
-        apply_contours_df = analysis.assign_clusters_id(principal_components,
-                                                        contours,
-                                                        self.centroids,
-                                                        num_pc=self.num_pc)
+        principal_components = analysis.pca_transform_contours(
+            contours,
+            self.mean_aligned_contour,
+            self.principal_directions
+        )
+        apply_contours_df = analysis.assign_clusters_id(
+            principal_components,
+            contours,
+            self.centroids,
+            n_pcs=self.n_pcs
+        )
         properties_df = properties_df.join(apply_contours_df)
         return properties_df
 
+    def fit_transform(self, properties_df):
+        """
+        Fit VAMPIRE model to dataset and transform that dataset.
+
+        Parameters
+        ----------
+        properties_df : DataFrame
+            DataFrame containing contour properties and raw contours.
+
+        Returns
+        -------
+        properties_df : DataFrame
+            DataFrame containing contour properties, raw contours,
+            normalized contours, cluster id, and min distance from
+            centroid.
+
+        """
+        self.fit(properties_df)
+        return self.transform(properties_df)
+
     def __eq__(self, other):
         """
         Test equality with another Vampire object.
 
         Parameters
         ----------
         other : vampire.model.Vampire
@@ -204,18 +231,18 @@
         -------
         equal : bool
 
         """
         equal = (
             # model hyperparameters
             self.model_name == other.model_name
-            and self.num_points == other.num_points
-            and self.num_coord == other.num_coord
-            and self.num_clusters == other.num_clusters
-            and self.num_pc == other.num_pc
+            and self.n_points == other.n_points
+            and self.n_coords == other.n_coords
+            and self.n_clusters == other.n_clusters
+            and self.n_pcs == other.n_pcs
             and self.random_state == other.random_state
             # contour info
             and np.allclose(self.mean_registered_contour, other.mean_registered_contour)
             and np.allclose(self.mean_aligned_contour, other.mean_aligned_contour)
             and np.allclose(self.contours, other.contours)
             # pca analysis info
             and np.allclose(self.principal_directions, other.principal_directions)
```

## vampire/pilot_draw.py

```diff
@@ -255,15 +255,15 @@
 
 # img_set = []
 # for radius in np.arange(20, 35, 5):
 #     img = circle_img(500, 500, int(radius))
 #     labeled_img = measure.label(img)
 #     img_set.append(labeled_img)
 # df = extraction.extract_properties_from_img_set(img_set)
-# vampire_model = model.Vampire('circles', num_clusters=2, random_state=1)
+# vampire_model = model.Vampire('circles', n_clusters=2, random_state=1)
 # vampire_model.build(df)
 # apply_properties_df = vampire_model.apply(df)
 # plot.set_plot_style()
 # plot.plot_representatives(vampire_model, apply_properties_df, alpha=0.5)
 # plot.plot_distribution_contour_dendrogram(vampire_model,
 #                                           apply_properties_df,
 #                                           height_ratio=[4, 1, 1])
@@ -271,15 +271,15 @@
 
 # img_set = []
 # for major in np.arange(10, 100, 5):
 #     img = rectangle_img(500, 500, int(major), 5)
 #     labeled_img = measure.label(img)
 #     img_set.append(labeled_img)
 # df = extraction.extract_properties_from_img_set(img_set)
-# vampire_model = model.Vampire('rectangles', num_clusters=5, random_state=1)
+# vampire_model = model.Vampire('rectangles', n_clusters=5, random_state=1)
 # vampire_model.build(df)
 # apply_properties_df = vampire_model.apply(df)
 # plot.set_plot_style()
 # plot.plot_representatives(vampire_model, apply_properties_df, alpha=0.5)
 # plot.plot_distribution_contour_dendrogram(vampire_model,
 #                                           apply_properties_df,
 #                                           height_ratio=[4, 1, 1])
@@ -288,15 +288,15 @@
 # img_set = []
 # for major in np.arange(10, 100, 5):
 #     for angle in np.arange(0, 90, 30):
 #         img = rectangle_img(500, 500, int(major), 5, rotation=angle)
 #         labeled_img = measure.label(img)
 #         img_set.append(labeled_img)
 # df = extraction.extract_properties_from_img_set(img_set)
-# vampire_model = model.Vampire('rectangles-rotated', num_clusters=5, random_state=1)
+# vampire_model = model.Vampire('rectangles-rotated', n_clusters=5, random_state=1)
 # vampire_model.build(df)
 # apply_properties_df = vampire_model.apply(df)
 # plot.set_plot_style()
 # plot.plot_representatives(vampire_model, apply_properties_df, alpha=0.5)
 # plot.plot_distribution_contour_dendrogram(vampire_model,
 #                                           apply_properties_df,
 #                                           height_ratio=[4, 1, 1])
@@ -306,17 +306,17 @@
 img_set = []
 for l1 in np.arange(20, 100, 10):
     for angle in np.arange(10, 180, 30):
         img = triangle_img(500, 500, int(l1), 15, theta=angle)
         labeled_img = measure.label(img)
         img_set.append(labeled_img)
 df = extraction.extract_properties_from_img_set(img_set)
-vampire_model = model.Vampire('triangles', num_clusters=5, random_state=1)
-vampire_model.build(df)
-apply_properties_df = vampire_model.apply(df)
+vampire_model = model.Vampire('triangles', n_clusters=5, random_state=1)
+vampire_model.fit(df)
+apply_properties_df = vampire_model.transform(df)
 plot.set_plot_style()
 plot.plot_representatives(vampire_model, apply_properties_df, alpha=0.5)
 plot.plot_distribution_contour_dendrogram(vampire_model,
                                           apply_properties_df,
                                           height_ratio=[4, 1, 1])
 plt.show()
```

## vampire/plot.py

```diff
@@ -44,20 +44,22 @@
         'legend.framealpha': 1,
         'legend.edgecolor': 'black',
         'legend.fancybox': False,
         'legend.fontsize': 14,
     })
 
 
-def save_fig(fig,
-             output_path,
-             fig_type,
-             extension='.png',
-             model_name=None,
-             apply_name=None):
+def save_fig(
+        fig,
+        output_path,
+        fig_type,
+        extension='.png',
+        model_name=None,
+        apply_name=None
+):
     """
     Save figure to local directory.
 
     Parameters
     ----------
     fig : Figure
         Figure to be saved.
@@ -113,28 +115,36 @@
     scipy.cluster.hierarchy.dendrogram
 
     """
     if ax is None:
         fig, ax = plt.subplots(figsize=fig_size)
 
     cluster.hierarchy.set_link_color_palette(['k'])
-    cluster.hierarchy.dendrogram(model.linkage_matrix,
-                                 ax=ax,
-                                 p=0,
-                                 truncate_mode='lastp',
-                                 orientation='bottom',
-                                 above_threshold_color='k')
+    cluster.hierarchy.dendrogram(
+        model.linkage_matrix,
+        ax=ax,
+        p=0,
+        truncate_mode='lastp',
+        orientation='bottom',
+        above_threshold_color='k'
+    )
     ax.axis('off')
     return ax
 
 
-def plot_contours(model, apply_properties_df=None,
-                  contour_scale=3,
-                  ax=None, fig_size=(6, 2),
-                  colors=None, alpha=1, lw=2):
+def plot_contours(
+        model,
+        apply_properties_df=None,
+        contour_scale=3,
+        ax=None,
+        fig_size=(6, 2),
+        colors=None,
+        alpha=1,
+        lw=2
+):
     """
     Plots mean contours.
 
     Parameters
     ----------
     model : vampire.model.Vampire
         Built VAMPIRE model.
@@ -166,53 +176,59 @@
         cluster_id_df = apply_properties_df[['cluster_id', 'distance_to_centroid']]
         labeled_contours_df = analysis.get_labeled_contours_df(contours, cluster_id_df)
         mean_cluster_contours = analysis.get_mean_cluster_contours(labeled_contours_df)
     if ax is None:
         fig, ax = plt.subplots(figsize=fig_size)
     if colors is None:
         colors = [plt.get_cmap('twilight')(cluster_i)
-                  for cluster_i in np.linspace(0.1, 0.9, model.num_clusters)]
+                  for cluster_i in np.linspace(0.1, 0.9, model.n_clusters)]
     elif type(colors) == str:
-        colors = [colors] * model.num_clusters
+        colors = [colors] * model.n_clusters
 
     x_first = 5  # offset of first contour
     x_offset = 10  # offset between contours
 
-    for i in range(model.num_clusters):
+    for i in range(model.n_clusters):
         # read in contour coordinates
-        x = mean_cluster_contours[i, :model.num_points]
-        y = mean_cluster_contours[i, model.num_points:]
+        x = mean_cluster_contours[i, :model.n_points]
+        y = mean_cluster_contours[i, model.n_points:]
         # form close shape when plotting
         x = np.append(x, x[0])
         y = np.append(y, y[0])
         # place shape into right location
         x = x * contour_scale + x_first + x_offset * i
         y = y * contour_scale
         # plot shape of objects corresponding to the branches
         ax.plot(x, y, color=colors[i], alpha=alpha, lw=lw)
     ax.axis('equal')
     ax.axis('off')
     return ax
 
 
-def plot_representatives(model, apply_properties_df,
-                         num_sample=10, random_state=None,
-                         ax=None, fig_size=(17, 2),
-                         colors=None, alpha=None, lw=None):
+def plot_representatives(
+        model, apply_properties_df,
+        num_sample=10,
+        random_state=None,
+        ax=None,
+        fig_size=(17, 2),
+        colors=None,
+        alpha=None,
+        lw=None
+):
     """
     Plots representative object contours.
 
     Parameters
     ----------
     model : vampire.model.Vampire
         Built VAMPIRE model.
     apply_properties_df : DataFrame, optional
         Properties output of VAMPIRE model applied to data.
     num_sample : int, optional
-        Number of sample drawn from each cluster. Default 10. If num_sample >
+        Number of sample drawn from each cluster. Default 10. If n_samples >
         number of total available samples in the smallest cluster, it is
         set to the that number.
     random_state : int, optional
         Random state of sampling representative contours.
     ax : Axes, optional
         Figure axis to be plotted on. Cannot be not None with ``output_path``
         at the same time.
@@ -230,17 +246,17 @@
     ax : matplotlib.axes.Axes
 
     """
     if ax is None:
         fig, ax = plt.subplots(figsize=fig_size)
     if colors is None:
         colors = [plt.get_cmap('twilight')(cluster_i)
-                  for cluster_i in np.linspace(0.1, 0.9, model.num_clusters)]
+                  for cluster_i in np.linspace(0.1, 0.9, model.n_clusters)]
     elif type(colors) == str:
-        colors = [colors] * model.num_clusters
+        colors = [colors] * model.n_clusters
 
     x_offset = 5  # move center of another cluster to new location
 
     # determine sample size
     cluster_id = apply_properties_df['cluster_id'].values
     unique, counts = np.unique(cluster_id, return_counts=True)
     max_num_sample = np.min(counts)
@@ -248,27 +264,29 @@
         num_sample = max_num_sample
 
     # sample contours from all clusters
     contours = np.vstack(apply_properties_df['normalized_contour'].to_numpy())
     cluster_id_df = apply_properties_df['cluster_id']
     labeled_contours_df = analysis.get_labeled_contours_df(contours, cluster_id_df)
     all_cluster_samples_df = labeled_contours_df.groupby('cluster_id') \
-        .sample(n=num_sample,
-                random_state=random_state)
+        .sample(
+        n=num_sample,
+        random_state=random_state
+    )
 
     # plotting each sample contour
-    for cluster_i in range(model.num_clusters):
+    for cluster_i in range(model.n_clusters):
         # sample contour in current cluster
         cluster_samples_df = all_cluster_samples_df[all_cluster_samples_df['cluster_id'] == cluster_i]
         cluster_samples = cluster_samples_df.drop(columns='cluster_id').values
         for sample_i in range(num_sample):
             # plot each sample contour by...
             # read in sample contour coordinates
-            x = cluster_samples[sample_i, :model.num_points]
-            y = cluster_samples[sample_i, model.num_points:]
+            x = cluster_samples[sample_i, :model.n_points]
+            y = cluster_samples[sample_i, model.n_points:]
             # form close shape when plotting
             x = np.append(x, x[0])
             y = np.append(y, y[0])
             # place shape into right location
             x = x + x_offset * cluster_i
             y = y
             # sample shape of objects corresponding to the clusters
@@ -301,22 +319,30 @@
 
     distribution = analysis.get_distribution(properties_df) * 100  # unit: percent
     num_clusters = len(distribution)
 
     x_first = 5  # offset of first contour
     x_offset = 10  # offset between contours
     width = x_offset / 2
-    x = np.arange(x_first,
-                  num_clusters * x_offset + x_offset / 2,
-                  x_offset)
-    colors = [plt.get_cmap('twilight')(cluster_i)
-              for cluster_i in np.linspace(0.1, 0.9, num_clusters)]
-    ax.bar(x=x, height=distribution,
-           color=colors,
-           align='center', width=width)
+    x = np.arange(
+        x_first,
+        num_clusters * x_offset + x_offset / 2,
+        x_offset
+    )
+    colors = [
+        plt.get_cmap('twilight')(cluster_i)
+        for cluster_i in np.linspace(0.1, 0.9, num_clusters)
+    ]
+    ax.bar(
+        x=x,
+        height=distribution,
+        color=colors,
+        align='center',
+        width=width
+    )
     ax.set_ylabel(f'Distribution [%]')
     ax.set_xticks([])  # clear x tick and tick labels
 
     # figure settings after plotting
     plt.tight_layout()
     return ax
 
@@ -344,16 +370,20 @@
     """
     fig, axs = plt.subplots(2, 1, figsize=fig_size, frameon=False, sharex='all')
     plot_dendrogram(model, ax=axs[1])
     plot_contours(model, ax=axs[0])
     return fig, axs
 
 
-def plot_distribution_contour(model, apply_properties_df=None,
-                              fig_size=(5, 5), height_ratio=(4, 1)):
+def plot_distribution_contour(
+        model,
+        apply_properties_df=None,
+        fig_size=(5, 5),
+        height_ratio=(4, 1)
+):
     """
     Plots the distribution of mean contours in a bar graph with labeling
     of mean contours.
 
     Parameters
     ----------
     model : vampire.model.Vampire
@@ -376,28 +406,36 @@
     axs : matplotlib.axes.Axes
 
     See Also
     --------
     plot_contours, plot_distribution
 
     """
-    fig, axs = plt.subplots(2, 1, figsize=fig_size, sharex='all',
-                            gridspec_kw={'height_ratios': height_ratio})
+    fig, axs = plt.subplots(
+        2, 1,
+        figsize=fig_size,
+        sharex='all',
+        gridspec_kw={'height_ratios': height_ratio}
+    )
     plot_contours(model, ax=axs[1])
     if apply_properties_df is None:
         plot_distribution(model.cluster_id_df, ax=axs[0])
     else:
         plot_distribution(apply_properties_df, ax=axs[0])
     plt.tight_layout()
     fig.subplots_adjust(hspace=0)
     return fig, axs
 
 
-def plot_distribution_contour_dendrogram(model, apply_properties_df=None,
-                                         fig_size=(5, 5), height_ratio=(4, 1, 1)):
+def plot_distribution_contour_dendrogram(
+        model,
+        apply_properties_df=None,
+        fig_size=(5, 5),
+        height_ratio=(4, 1, 1)
+):
     """
     Plots the distribution of mean contours in a bar graph with labeling
     of mean contours and their dendrogram.
 
     Parameters
     ----------
     model : vampire.model.Vampire
@@ -422,25 +460,33 @@
     See Also
     --------
     plot_dendrogram, plot_contours, plot_distribution
 
     """
     # figure structure
     height_ratio = list(height_ratio)  # prevent mutable default argument
-    fig, axs = plt.subplots(3, 1, figsize=fig_size, sharex='all',
-                            gridspec_kw={'height_ratios': height_ratio})
+    fig, axs = plt.subplots(
+        3, 1,
+        figsize=fig_size,
+        sharex='all',
+        gridspec_kw={'height_ratios': height_ratio}
+    )
 
     if apply_properties_df is None:  # build model plot
         plot_dendrogram(model, ax=axs[2])
         plot_contours(model, ax=axs[1])
         plot_distribution(model.cluster_id_df, ax=axs[0])
     else:  # apply model plot
         plot_dendrogram(model, ax=axs[2])
         plot_contours(model, ax=axs[1])
-        plot_contours(model, apply_properties_df, ax=axs[1],
-                      colors='black', alpha=0.3)
+        plot_contours(
+            model,
+            apply_properties_df, ax=axs[1],
+            colors='black',
+            alpha=0.3
+        )
         plot_distribution(apply_properties_df, ax=axs[0])
 
     # figure settings after plotting
     plt.tight_layout()
     fig.subplots_adjust(hspace=0)
     return fig, axs
```

## vampire/processing.py

```diff
@@ -1,82 +1,82 @@
 import numpy as np
 from scipy import interpolate
 
 from . import amath
 
 
-def sample_contour(contour, num_sample_points):
+def sample_contour(contour, n_sample_pts):
     """
     Returns sample points of contour using B-spline.
 
     Interpolate given coordinates of an object contour using B-spline,
-    then fit `num_sample_points` equidistant points to the B-spline.
+    then fit `n_sample_pts` equidistant points to the B-spline.
 
     Parameters
     ----------
     contour : ndarray
         x and y coordinates of object contour, with shape (2, n).
-    num_sample_points : int
+    n_sample_pts : int
         Number of sample points after resample.
 
     Returns
     -------
     sampled_contour : ndarray
-        `num_sample_points` equidistant contour sample points,
-        with shape (2, num_sample_points).
+        `n_sample_pts` equidistant contour sample points,
+        with shape (2, n_sample_pts).
 
     """
     x, y = contour
     # check if object shape is closed
     if np.all(contour[:, 0] != contour[:, -1]):
         x = np.append(x, x[0])
         y = np.append(y, y[0])
     distance = np.sqrt(np.diff(x)**2 + np.diff(y)**2)
     # pad arbitrary number to give same length as x and y
     # can be arbitrary since cumulative sum is taken, identical result
     distance = np.append([1], distance)
     cum_distance = np.cumsum(distance)
-    sample_points = np.linspace(cum_distance[0], cum_distance[-1], num_sample_points)
+    sample_points = np.linspace(cum_distance[0], cum_distance[-1], n_sample_pts)
     # interpolate the data points using b-spline
     x_spliner = interpolate.splrep(cum_distance, x, s=0)
     y_spliner = interpolate.splrep(cum_distance, y, s=0)
     # fit points to the b-spline
     x_samples = interpolate.splev(sample_points, x_spliner)
     y_samples = interpolate.splev(sample_points, y_spliner)
     sampled_contour = np.vstack([x_samples, y_samples])
     return sampled_contour
 
 
-def sample_contours(contours, num_points=50):
+def sample_contours(contours, n_points=50):
     """
     Returns sampled contours using B-spline.
 
     Parameters
     ----------
     contours : list[ndarray]
-        List of contour coordinates, list with length num_contours,
-        ndarray with shape (2, num_points).
-    num_points : int, optional
+        List of contour coordinates, list with length n_contours,
+        ndarray with shape (2, n_points).
+    n_points : int, optional
         Number of sample points of object contour. Defaults to 50.
 
     Returns
     -------
     sampled_contours : list[ndarray]
-        Sampled contours, list with length num_contours, ndarray with
-        shape (2, num_points).
+        Sampled contours, list with length n_contours, ndarray with
+        shape (2, n_points).
 
     See Also
     --------
     sample_contour
 
     """
-    num_contours = len(contours)
+    n_contours = len(contours)
     sampled_contours = []
-    for i in range(num_contours):
-        sampled_contour = sample_contour(contours[i], num_points)
+    for i in range(n_contours):
+        sampled_contour = sample_contour(contours[i], n_points)
         sampled_contours.append(sampled_contour)
     return sampled_contours
 
 
 # noinspection PyPep8Naming
 def register_contour(contour):
     r"""
@@ -174,30 +174,30 @@
     .. rubric:: **Re-orientate the data points**
 
     Although the contour data points are ordered so that a closed shape can
     be drawn by connecting each point, the starting data point could start
     at random location of the shape and proceed in either clockwise or
     counterclockwise directions. Here, we reorient the data points so that
     the contour has *positive orientation*. Meaning, the data points starts
-    at the point that makes the smallest angle with the major axis at the
+    at the point that makes the smallest angle with the major axis on the
     right side of the shape, and the data points goes in counterclockwise
     direction.
 
     We first reorder the data points by finding the appropriate starting
     point. The major axis in this case is :math:`x = 0` since the data is
     mean-subtracted. The starting data point that makes the smallest angle
-    with the major axis at the right side will have minimum absolute angle
+    with the major axis on the right side will have minimum absolute angle
     defined by polar coordinates. The angles of points with respect to the
     origin is given by
 
     .. math::
 
         \theta_i = \arctan\left(\dfrac{y_i}{x_i}\right),
 
-    and the index of point with smallest angle is
+    and the index of point with the smallest angle is
 
     .. math::
 
         \mathrm{argmin}_{i} \vert\theta_i\vert.
 
     We then check if the contour goes in the counterclockwise direction.
     Assuming the object shape is locally convex, counterclockwise contours
@@ -230,15 +230,18 @@
     R = np.sqrt(np.sum(B**2) / N)
     B_prime = B / R
     # principal component analysis
     V, T, d = amath.pca(B_prime, method='svd')
     # let data point start at the right close to the major axis
     theta = np.arctan2(T[:, 1], T[:, 0])
     starting_index = np.argmin(np.abs(theta))
-    reorder_index = np.hstack([np.arange(starting_index, N), np.arange(starting_index)])
+    reorder_index = np.hstack([
+        np.arange(starting_index, N),
+        np.arange(starting_index)
+    ])
     T = T[reorder_index, :]
     theta = theta[reorder_index]
     # make contour positively oriented
     if theta[0] > theta[4]:  # assume locally convex, 4 is arbitrary choice
         T = np.flip(T, axis=0)
     registered_contour = T.T
     return registered_contour
@@ -247,50 +250,50 @@
 def register_contours(contours):
     """
     Returns registered contours to their principal components.
 
     Parameters
     ----------
     contours : list[ndarray]
-        List of contour coordinates, list with length num_contours,
-        ndarray with shape (2, num_points).
+        List of contour coordinates, list with length n_contours,
+        ndarray with shape (2, n_points).
 
     Returns
     -------
     registered_contours : list[ndarray]
-        Registered contours, list with length num_contours, ndarray with
-        shape (2, num_points).
+        Registered contours, list with length n_contours, ndarray with
+        shape (2, n_points).
 
     See Also
     --------
     register_contour
 
     """
-    num_contours = len(contours)
+    n_contours = len(contours)
     registered_contours = []
-    for i in range(num_contours):
+    for i in range(n_contours):
         registered_contour = register_contour(contours[i])
         registered_contours.append(registered_contour)
     return registered_contours
 
 
 def get_mean_registered_contour(registered_contours):
     """
     Compute mean of registered contours.
 
     Parameters
     ----------
     registered_contours : list[ndarray]
-        Registered contours, list with length num_contours, ndarray with
-        shape (2, num_points).
+        Registered contours, list with length n_contours, ndarray with
+        shape (2, n_points).
 
     Returns
     -------
     mean_registered_contour : ndarray
-        Mean of registered contours, with shape (2, num_points).
+        Mean of registered contours, with shape (2, n_points).
 
     """
     registered_contours_flat = np.asarray(registered_contours)
     mean_registered_contour = np.mean(registered_contours_flat, axis=0)
     return mean_registered_contour
 
 
@@ -363,72 +366,78 @@
     A_bar = mean_contour
     N = contour.shape[1]
     SSD_best = np.linalg.norm(A - A_bar) ** 2
 
     best_index = 0
     for i in range(N):
         starting_index = i
-        reorder_index = np.hstack([np.arange(starting_index, N), np.arange(starting_index)])
+        reorder_index = np.hstack([
+            np.arange(starting_index, N),
+            np.arange(starting_index)]
+        )
         A_i = A[:, reorder_index]
         R = amath.get_rotation_matrix(A_i, A_bar)
         A_prime = R @ A_i
         SSD_current = np.linalg.norm(A_prime - A_bar) ** 2
         if SSD_current < SSD_best:
             best_index = i
             SSD_best = SSD_current
 
-    reorder_index = np.hstack([np.arange(best_index, N), np.arange(best_index)])
+    reorder_index = np.hstack([
+        np.arange(best_index, N),
+        np.arange(best_index)
+    ])
     A_i = A[:, reorder_index]
     R = amath.get_rotation_matrix(A_i, A_bar)
     A_prime = R @ A_i
     aligned_contour = A_prime
     return aligned_contour
 
 
 def align_contours(contours, mean_contour):
     """
     Returns aligned contours to their mean.
 
     Parameters
     ----------
     contours : list[ndarray]
-        List of contour coordinates. List with length num_contours;
-        ndarray with shape (2, num_points).
+        List of contour coordinates. List with length n_contours;
+        ndarray with shape (2, n_points).
     mean_contour : ndarray
-        Mean of registered contours, with shape (2, num_points).
+        Mean of registered contours, with shape (2, n_points).
 
     Returns
     -------
     aligned_contours_flat : ndarray
-        Flattened aligned contours, with shape (num_contours, 2*num_points).
+        Flattened aligned contours, with shape (n_contours, 2*n_points).
 
     See Also
     --------
     align_contour
 
     """
-    num_contours = len(contours)
+    n_contours = len(contours)
     aligned_contours_flat = []
-    for j in range(num_contours):
+    for j in range(n_contours):
         aligned_contour = align_contour(contours[j], mean_contour)
         aligned_contours_flat.append(aligned_contour.reshape(-1))
     return aligned_contours_flat
 
 
 def get_mean_aligned_contour(aligned_contours_flat):
     """
     Compute mean of aligned contours.
 
     Parameters
     ----------
     aligned_contours_flat : ndarray
-        Flattened aligned contours, with shape (num_contours, 2*num_points).
+        Flattened aligned contours, with shape (n_contours, 2*n_points).
 
     Returns
     -------
     mean_contour_flat : ndarray
-        Flattened mean contours, with size 2*num_points.
+        Flattened mean contours, with size 2*n_points.
 
     """
     aligned_contours_flat = np.asarray(aligned_contours_flat)
     mean_contour_flat = np.mean(aligned_contours_flat, axis=0)
     return mean_contour_flat
```

## vampire/quickstart.py

```diff
@@ -3,15 +3,15 @@
 
 import numpy as np
 import pandas as pd
 
 from . import extraction, model, plot, util
 
 
-def _check_prohibited_char(text, input_type='path'):
+def _check_char(text, input_type='path'):
     r"""
     Checks if path contains characters prohibited by the operating system.
 
     Parameters
     ----------
     text : str
         Path of file or directory.
@@ -33,27 +33,33 @@
 
     """
     if input_type == 'path':
         prohibited_chars = [',', '*', '"', '<', '>', '|']
     elif input_type == 'file':
         prohibited_chars = ['\\', '/', ',', ':', '*', '"', '<', '>', '|']
     else:
-        raise ValueError('Unrecognized input_type: {input_type}'
-                         'Expect one in {"path", "file"}')
-    have_prohibited_char = any(prohibited_char in text
-                               for prohibited_char in prohibited_chars)
+        raise ValueError(
+            'Unrecognized input_type: {input_type}'
+            'Expect one in {"path", "file"}'
+        )
+    have_prohibited_char = any(
+        prohibited_char in text
+        for prohibited_char in prohibited_chars
+    )
     if have_prohibited_char:
-        raise ValueError(f'Filename-related entry of {text} contains prohibited character(s). \n'
-                         'Expect a model name without any of the following characters: \n'
-                         '\\  /  ,  :  *  "  <  >  |')
+        raise ValueError(
+            f'Filename-related entry of {text} contains prohibited character(s). \n'
+            'Expect a model name without any of the following characters: \n'
+            '\\  /  ,  :  *  "  <  >  |'
+        )
 
 
 def _parse_filter_info(filter_info):
     """
-    Parse optional column(s) of input DataFrame to `build_models` and
+    Parse optional column(s) of input DataFrame to `fit_models` and
     `apply_models`.
 
     Checks argument requirements.
 
     Parameters
     ----------
     filter_info : ndarray
@@ -74,226 +80,136 @@
     """
     if filter_info.size == 0:
         return np.array([], dtype=str)
     else:
         return filter_info[filter_info != 'nan']
 
 
-def _build_models_check_df(img_info_df):
+def fit_model(
+    img_set_path,
+    output_path,
+    model_name,
+    n_points,
+    n_clusters,
+    n_pcs,
+    filter_info,
+    write_contour=False,
+    random_state=None,
+    savefig=True,
+):
     """
-    Checks if input DataFrame to `build_models` has the appropriate shape.
+    Fits VAMPIRE model to one image set.
 
     Parameters
     ----------
-    img_info_df : DataFrame
-        Input to `build_models`. Contains all information about image sets
-        to be analyzed.
-
-    Raises
-    ------
-    ValueError
-        Empty DataFrame without information in rows.
-    ValueError
-        DataFrame does not contain required columns specified in the doc.
-
-    """
-    num_img_sets, num_args = img_info_df.shape
-    if num_img_sets == 0:
-        raise ValueError('Input DataFrame is empty. Expect at least one row.')
-    if num_args < 6:  # 5 cols required by doc
-        raise ValueError('Input DataFrame does not have enough number of columns. \n'
-                         'Expect required 6 columns in order: img_set_path, output_path, '
-                         'model_name, num_points, num_clusters, num_pc.')
-
-
-def _build_models_check_required_info(required_info):
-    """
-    Parse required columns of input DataFrame to `build_models`.
-
-    Checks argument requirements and sets default arguments.
-
-    Parameters
-    ----------
-    required_info : Series
-        One row of required columns (1-5) of input DataFrame
-        ``img_info_df``.
-
-    Returns
-    -------
     img_set_path : str
-        Path to the directory containing the image set(s) used to build model.
+        Path to the directory containing the image set(s) used to fit model.
     output_path : str
         Path of the directory used to output model and figures. Defaults to
         ``img_set_path``.
     model_name : str
         Name of the model. Defaults to time of function call.
-    num_points : int
+    n_points : int
         Number of sample points of object contour. Defaults to 50.
-    num_clusters : int
-        Number of clusters of K-means clustering. Defaults to 5.
-    num_pc : int
-        Number of principal components kept for analysis. Defaults to None.
-
-    Raises
-    ------
-    FileNotFoundError
-        If ``img_set_path`` does not exist.
-
-    """
-    # unpack args
-    img_set_path, output_path, model_name, num_points, num_clusters, num_pc = required_info
-
-    # img_set_path
-    if os.path.isdir(img_set_path):
-        img_set_path = os.path.normpath(img_set_path)
-    else:
-        raise FileNotFoundError(f'Input DataFrame column 1 gives non-existing directory: \n'
-                                f'{img_set_path} \n'
-                                'Expect an existing directory with images used to build model.')
-
-    # output_path
-    if pd.isna(output_path):
-        output_path = os.path.normpath(img_set_path)  # default
-    else:
-        _check_prohibited_char(output_path)
-        output_path = os.path.normpath(output_path)
-        if not os.path.isdir(output_path):
-            os.mkdir(output_path)
-
-    # model_name
-    if pd.isna(model_name):
-        time_stamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
-        model_name = time_stamp
-    else:
-        _check_prohibited_char(model_name, 'file')
-
-    # num_points
-    if pd.isna(num_points):
-        num_points = 50
-    else:
-        num_points = int(num_points)
-
-    # num_clusters
-    if pd.isna(num_clusters):
-        num_clusters = 5
-    else:
-        num_clusters = int(num_clusters)
-
-    # num_pc
-    if pd.isna(num_pc):
-        num_pc = None
-    else:
-        num_pc = int(num_pc)
-
-    return img_set_path, output_path, model_name, num_points, num_clusters, num_pc
-
-
-def build_model(img_set_path, output_path,
-                model_name, num_points,
-                num_clusters, num_pc, filter_info,
-                random_state=None, savefig=True):
-    """
-    Builds VAMPIRE model to one image set.
-
-    Parameters
-    ----------
-    img_set_path : str
-        Path to the directory containing the image set(s) used to build model.
-    output_path : str
-        Path of the directory used to output model and figures. Defaults to
-        ``img_set_path``.
-    model_name : str
-        Name of the model. Defaults to time of function call.
-    num_points : int
-        Number of sample points of object contour. Defaults to 50.
-    num_clusters : int
+    n_clusters : int
         Number of clusters of K-means clustering. Defaults to 5. Recommended
         range [2, 10].
-    num_pc : int or None
+    n_pcs : int or None
         Number of principal components kept for analysis. Default to keeping
         those that explains 95% of total variance. Recommended to adjust
         after analyzing scree plot.
+    write_contour : bool, optional
+        Whether write and save raw contour coordinates.
     filter_info : ndarray
         Regex filter(s) of image filenames to be analyzed.
         Empty if no filter needed.
     random_state : int, optional
         Random state of random processes.
     savefig : bool, optional
         Whether save distribution contour dendrogram.
 
     See Also
     --------
-    build_models : Building multiple models using different images/conditions.
+    fit_models : Fitting multiple models using different images/conditions.
 
     """
     # get data
-    properties_df = extraction.extract_properties(img_set_path,
-                                                  filter_info,
-                                                  write=True)
-    # build model
-    vampire_model = model.Vampire(model_name,
-                                  num_points=num_points,
-                                  num_clusters=num_clusters,
-                                  num_pc=num_pc,
-                                  random_state=random_state)
-    vampire_model.build(properties_df)
+    properties_df = extraction.extract_properties(
+        img_set_path,
+        filter_info,
+        write=True,
+        write_contour=write_contour,
+    )
+    # fit model
+    vampire_model = model.Vampire(
+        model_name,
+        n_points=n_points,
+        n_clusters=n_clusters,
+        n_pcs=n_pcs,
+        random_state=random_state,
+    )
+    vampire_model.fit(properties_df)
     # write model
-    model_output_path = util.get_model_pickle_path(output_path,
-                                                   filter_info,
-                                                   vampire_model)
+    model_output_path = util.get_model_pickle_path(
+        output_path,
+        filter_info,
+        vampire_model,
+    )
     util.write_pickle(model_output_path, vampire_model)
     # plot result
     if savefig:
         plot.set_plot_style()
         fig, axs = plot.plot_distribution_contour_dendrogram(vampire_model)
-        plot.save_fig(fig,
-                      output_path,
-                      'shape_mode',
-                      '.png',
-                      model_name)
+        plot.save_fig(
+            fig,
+            output_path,
+            'shape_mode',
+            '.png',
+            model_name
+        )
     return vampire_model
 
 
-def build_models(img_info_df, random_state=None, savefig=True):
+def fit_models(img_info_df, random_state=None, savefig=True):
     """
-    Builds all models from the input info of image sets.
+    Fits all models from the input info of image sets.
 
     Parameters
     ----------
     img_info_df : DataFrame
         Contains all information about image sets to be analyzed. See notes.
     random_state : int, optional
         Random state of random processes.
     savefig : bool, optional
         Whether save distribution contour dendrogram.
 
     Notes
     -----
-    Learn more about :ref:`basics <build_basics>` and
-    :ref:`advanced <build_advanced>` input requirement
+    Learn more about :ref:`basics <fit_basics>` and
+    :ref:`advanced <fit_advanced>` input requirement
     and examples. Below is a general description.
 
     .. rubric:: **Required columns of** ``img_info_df`` **(col 1-6)**
 
     The input DataFrame ``img_info_df`` must contain, *in order*, the 6
     required columns of
 
     img_set_path : str
-        Path to the directory containing the image set(s) used to build model.
+        Path to the directory containing the image set(s) used to fit model.
     output_path : str
         Path of the directory used to output model and figures. Defaults to
         ``img_set_path``.
     model_name : str, default
         Name of the model. Defaults to time of function call.
-    num_points : int, default
+    n_points : int, default
         Number of sample points of object contour. Defaults to 50.
-    num_clusters : int, default
+    n_clusters : int, default
         Number of clusters of K-means clustering. Defaults to 5. Recommended
         range [2, 10].
-    num_pc : int, default
+    n_pcs : int, default
         Number of principal components kept for analysis. Default to keeping
         those that explains 95% of total variance. Recommended to adjust
         after analyzing scree plot.
 
     in the first 5 columns.
     The default values are used in default columns when (1) the space is left
     blank in ``csv``/``excel`` file before converting to DataFrame, or
@@ -324,153 +240,141 @@
 
     .. tip::
        The column names of optional columns does not affect the analysis.
        The values in the columns only serves as filters to images to be
        analyzed.
 
     """
-    _build_models_check_df(img_info_df)
-    num_img_set, num_args = img_info_df.shape
 
-    for row_i in range(num_img_set):
+    def check_info_df(img_info_df):
+        """
+        Checks if input DataFrame to `fit_models` has the appropriate shape.
+
+        Raises
+        ------
+        ValueError
+            Empty DataFrame without information in rows.
+        ValueError
+            DataFrame does not contain required columns specified in the doc.
+
+        """
+        n_img_sets, n_args = img_info_df.shape
+        if n_img_sets == 0:
+            raise ValueError('Input DataFrame is empty. Expect at least one row.')
+        if n_args < 6:  # 5 cols required by doc
+            raise ValueError(
+                'Input DataFrame does not have enough number of columns. \n'
+                'Expect required 6 columns in order: img_set_path, output_path, '
+                'model_name, n_points, n_clusters, n_pcs.'
+            )
+
+    def check_required_info(required_info):
+        """
+        Parse required columns of input DataFrame to `fit_models`.
+
+        Checks argument requirements and sets default arguments.
+
+        Raises
+        ------
+        FileNotFoundError
+            If ``img_set_path`` does not exist.
+
+        """
+        # unpack args
+        img_set_path, output_path, model_name, n_points, n_clusters, n_pcs = required_info
+
+        # img_set_path
+        if os.path.isdir(img_set_path):
+            img_set_path = os.path.normpath(img_set_path)
+        else:
+            raise FileNotFoundError(
+                f'Input DataFrame column 1 gives non-existing directory: \n'
+                f'{img_set_path} \n'
+                'Expect an existing directory with images used to fit model.'
+            )
+
+        # output_path
+        if pd.isna(output_path):
+            output_path = os.path.normpath(img_set_path)  # default
+        else:
+            _check_char(output_path)
+            output_path = os.path.normpath(output_path)
+            if not os.path.isdir(output_path):
+                os.mkdir(output_path)
+
+        # model_name
+        if pd.isna(model_name):
+            time_stamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
+            model_name = time_stamp
+        else:
+            _check_char(model_name, 'file')
+
+        # n_points
+        if pd.isna(n_points):
+            n_points = 50
+        else:
+            n_points = int(n_points)
+
+        # n_clusters
+        if pd.isna(n_clusters):
+            n_clusters = 5
+        else:
+            n_clusters = int(n_clusters)
+
+        # n_pcs
+        if pd.isna(n_pcs):
+            n_pcs = None
+        else:
+            n_pcs = int(n_pcs)
+
+        return img_set_path, output_path, model_name, n_points, n_clusters, n_pcs
+
+    # start of main function
+    check_info_df(img_info_df)
+    n_img_set, n_args = img_info_df.shape
+
+    for row_i in range(n_img_set):
         # parse arguments
         img_info = img_info_df.iloc[row_i, :]
         required_info = img_info[:6]  # 6 cols expected in doc
         filter_info = img_info[6:].values.astype(str)
         (img_set_path,
             output_path,
             model_name,
-            num_points,
-            num_clusters,
-            num_pc) = _build_models_check_required_info(required_info)
+            n_points,
+            n_clusters,
+            n_pcs) = check_required_info(required_info)
         filter_info = _parse_filter_info(filter_info)
-        # build model
-        build_model(img_set_path,
-                    output_path,
-                    model_name,
-                    num_points,
-                    num_clusters,
-                    num_pc,
-                    filter_info,
-                    random_state=random_state,
-                    savefig=savefig)
-    return
-
-
-def _apply_models_check_df(img_info_df):
-    """
-    Checks if input DataFrame to `apply_models` has the appropriate shape.
-
-    Parameters
-    ----------
-    img_info_df : DataFrame
-        Input to `apply_models`. Contains all information about image sets
-        to be analyzed.
-
-    Raises
-    ------
-    ValueError
-        Empty DataFrame without information in rows.
-    ValueError
-        DataFrame does not contain required columns specified in the doc.
-
-    """
-    num_img_set, num_args = img_info_df.shape
-    if num_img_set == 0:
-        raise ValueError('Input DataFrame is empty. Expect at least one row.')
-    if num_args < 4:  # 4 cols required by doc
-        raise ValueError('Input DataFrame does not have enough number of columns. \n'
-                         'Expect required 3 columns in order: img_set_path, model_path, '
-                         'output_path.')
+        # fit model
+        fit_model(
+            img_set_path,
+            output_path,
+            model_name,
+            n_points,
+            n_clusters,
+            n_pcs,
+            filter_info,
+            random_state=random_state,
+            savefig=savefig
+        )
     return
 
 
-def _apply_models_check_required_info(required_info):
+def transform_dataset(
+    img_set_path,
+    model_path,
+    output_path,
+    img_set_name,
+    filter_info,
+    write_csv=True,
+    write_contour=False,
+    savefig=True,
+):
     """
-    Parse required columns of input DataFrame to `apply_models`.
-
-    Parameters
-    ----------
-    required_info : Series
-        One row of required columns (1-4) of input DataFrame
-        ``img_info_df``.
-
-    Returns
-    -------
-    img_set_path : str
-        Path to the directory containing the image set(s) used to apply model.
-    model_path : str
-        Path to the pickle file that stores model information.
-    output_path : str
-        Path of the directory used to output model and figures. Defaults to
-        ``img_set_path``.
-    img_set_name : str
-        Name of the image set being applied to.
-        Defaults to time of function call.
-
-    Raises
-    ------
-    FileNotFoundError
-        If ``img_set_path`` does not exist.
-    FileNotFoundError
-        If ``model_path`` does not exist.
-    ValueError
-        If ``model_path`` is not a ``pickle`` file.
-
-    """
-    # unpack args
-    img_set_path, model_path, output_path, img_set_name = required_info
-
-    # img_set_path
-    if os.path.isdir(img_set_path):
-        img_set_path = os.path.normpath(img_set_path)
-    else:
-        raise FileNotFoundError(f'Input DataFrame column 1 gives non-existing directory: \n'
-                                f'{img_set_path} \n'
-                                'Expect an existing directory with images used to apply model.')
-
-    # model_path
-    if os.path.isfile(model_path):
-        filename, extension = os.path.splitext(model_path)
-        if extension == '.pickle':
-            model_path = os.path.normpath(model_path)
-        else:
-            raise ValueError(f'Input DataFrame column 2 gives non-pickle file: \n'
-                             f'{model_path} \n'
-                             'Expect an existing pickle file for model information.')
-    else:
-        raise FileNotFoundError(f'Input DataFrame column 2 gives non-existing file: \n'
-                                f'{model_path} \n'
-                                'Expect an existing pickle file for model information.')
-
-    # output_path
-    if pd.isna(output_path):
-        output_path = os.path.normpath(img_set_path)
-    else:
-        _check_prohibited_char(output_path)
-        output_path = os.path.normpath(output_path)
-        if not os.path.isdir(output_path):
-            os.mkdir(output_path)
-
-    # img_set_name
-    if pd.isna(img_set_name):
-        time_stamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
-        img_set_name = time_stamp
-    else:
-        _check_prohibited_char(img_set_name, 'file')
-
-    return img_set_path, model_path, output_path, img_set_name
-
-
-def apply_model(img_set_path, model_path,
-                output_path, img_set_name,
-                filter_info, write_csv=True,
-                savefig=True):
-    """
-     Apply VAMPIRE model to one image set.
+    Apply VAMPIRE model to one image set.
 
     Parameters
     ----------
     img_set_path : str
         Path to the directory containing the image set(s) used to apply model.
     model_path : str
         Path to the pickle file that stores model information.
@@ -482,89 +386,105 @@
         Defaults to time of function call.
     filter_info : ndarray
         Regex filter(s) of image filenames to be analyzed.
         Empty if no filter needed.
     write_csv : bool, optional
         Whether write apply model data to csv.
         Could be time-consuming if csv is large.
+    write_contour : bool, optional
+        Whether write and save raw contour coordinates.
     savefig : bool, optional
         Whether save distribution contour dendrogram.
 
     See Also
     --------
     apply_models : Apply multiple models using different images/conditions.
 
     """
     # get model
     vampire_model = util.read_pickle(model_path)
     # get data
-    properties_df = extraction.extract_properties(img_set_path,
-                                                  filter_info,
-                                                  write=True)
+    properties_df = extraction.extract_properties(
+        img_set_path,
+        filter_info,
+        write=True,
+        write_contour=write_contour,
+    )
     # apply model
-    apply_properties_df = vampire_model.apply(properties_df)
+    apply_properties_df = vampire_model.transform(properties_df)
     # write apply model data
-    properties_pickle_path = util.get_apply_properties_pickle_path(output_path,
-                                                                   filter_info,
-                                                                   vampire_model,
-                                                                   img_set_name)
+    properties_pickle_path = util.get_apply_properties_pickle_path(
+        output_path,
+        filter_info,
+        vampire_model,
+        img_set_name
+    )
     util.write_pickle(properties_pickle_path, apply_properties_df)
     # plot result
     if savefig:
         plot.set_plot_style()
-        fig, axs = plot.plot_distribution_contour_dendrogram(vampire_model,
-                                                             apply_properties_df)
-        plot.save_fig(fig,
-                      output_path,
-                      'shape_mode',
-                      '.png',
-                      vampire_model.model_name,
-                      img_set_name)
+        fig, axs = plot.plot_distribution_contour_dendrogram(
+            vampire_model,
+            apply_properties_df
+        )
+        plot.save_fig(
+            fig,
+            output_path,
+            'shape_mode',
+            '.png',
+            vampire_model.model_name,
+            img_set_name
+        )
 
     # write apply model data to csv
     # time-consuming if csv is large
     if write_csv:
-        properties_csv_path = util.get_apply_properties_csv_path(output_path,
-                                                                 filter_info,
-                                                                 vampire_model,
-                                                                 img_set_name)
-        apply_properties_df.drop(['raw_contour',
-                                  'normalized_contour'],
-                                 axis=1) \
-                           .to_csv(properties_csv_path, index=False)
+        properties_csv_path = util.get_apply_properties_csv_path(
+            output_path,
+            filter_info,
+            vampire_model,
+            img_set_name
+        )
+        apply_properties_df.drop(
+            ['raw_contour', 'normalized_contour'],
+            axis=1
+        ).to_csv(
+            properties_csv_path,
+            index=False
+        )
     return apply_properties_df
 
 
-def apply_models(img_info_df, write_csv=True, savefig=True):
+def transform_datasets(img_info_df, write_csv=True, savefig=True):
     """
     Applies all models from the input info of image sets.
 
     Parameters
     ----------
     img_info_df : DataFrame
         Contains all information about image sets to be analyzed. See notes.
     write_csv : bool, optional
-        Whether write apply model data to csv.
+        Whether write transformed data to csv.
         Could be time-consuming if csv is large.
     savefig : bool, optional
         Whether save distribution contour dendrogram.
 
     Notes
     -----
-    Learn more about :ref:`basics <apply_basics>` and
-    :ref:`advanced <apply_advanced>` input requirement
+    Learn more about :ref:`basics <transform_basics>` and
+    :ref:`advanced <transform_advanced>` input requirement
     and examples. Below is a general description.
 
     .. rubric:: **Required columns of** ``img_info_df`` **(col 1-4)**
 
     The input DataFrame ``img_info_df`` must contain, *in order*, the 4
     required columns of
 
     img_set_path : str
-        Path to the directory containing the image set(s) used to apply model.
+        Path to the directory containing the image set(s) used to transform data.
     model_path : str
         Path to the pickle file that stores model information.
     output_path : str
         Path of the directory used to output model and figures. Defaults to
         ``img_set_path``.
     img_set_name : str, default
         Name of the image set being applied to.
@@ -600,28 +520,118 @@
 
     .. tip::
        The column names of optional columns does not affect the analysis.
        The values in the columns only serves as filters to images to be
        analyzed.
 
     """
-    _apply_models_check_df(img_info_df)
-    num_img_set, num_args = img_info_df.shape
-    for row_i in range(num_img_set):
+
+    def check_info_df(img_info_df):
+        """
+        Checks if input DataFrame to `apply_models` has the appropriate shape.
+
+        Raises
+        ------
+        ValueError
+            Empty DataFrame without information in rows.
+        ValueError
+            DataFrame does not contain required columns specified in the doc.
+
+        """
+        n_img_set, n_args = img_info_df.shape
+        if n_img_set == 0:
+            raise ValueError('Input DataFrame is empty. Expect at least one row.')
+        if n_args < 4:  # 4 cols required by doc
+            raise ValueError(
+                'Input DataFrame does not have enough number of columns. \n'
+                'Expect required 3 columns in order: img_set_path, model_path, '
+                'output_path.'
+            )
+        return
+
+    def check_required_info(required_info):
+        """
+        Parse required columns of input DataFrame to `apply_models`.
+
+        Raises
+        ------
+        FileNotFoundError
+            If ``img_set_path`` does not exist.
+        FileNotFoundError
+            If ``model_path`` does not exist.
+        ValueError
+            If ``model_path`` is not a ``pickle`` file.
+
+        """
+        # unpack args
+        img_set_path, model_path, output_path, img_set_name = required_info
+
+        # img_set_path
+        if os.path.isdir(img_set_path):
+            img_set_path = os.path.normpath(img_set_path)
+        else:
+            raise FileNotFoundError(
+                f'Input DataFrame column 1 gives non-existing directory: \n'
+                f'{img_set_path} \n'
+                'Expect an existing directory with images used to apply model.'
+            )
+
+        # model_path
+        if os.path.isfile(model_path):
+            filename, extension = os.path.splitext(model_path)
+            if extension == '.pickle':
+                model_path = os.path.normpath(model_path)
+            else:
+                raise ValueError(
+                    f'Input DataFrame column 2 gives non-pickle file: \n'
+                    f'{model_path} \n'
+                    'Expect an existing pickle file for model information.'
+                )
+        else:
+            raise FileNotFoundError(
+                f'Input DataFrame column 2 gives non-existing file: \n'
+                f'{model_path} \n'
+                'Expect an existing pickle file for model information.'
+            )
+
+        # output_path
+        if pd.isna(output_path):
+            output_path = os.path.normpath(img_set_path)
+        else:
+            _check_char(output_path)
+            output_path = os.path.normpath(output_path)
+            if not os.path.isdir(output_path):
+                os.mkdir(output_path)
+
+        # img_set_name
+        if pd.isna(img_set_name):
+            time_stamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
+            img_set_name = time_stamp
+        else:
+            _check_char(img_set_name, 'file')
+
+        return img_set_path, model_path, output_path, img_set_name
+
+    # start of main function
+    check_info_df(img_info_df)
+    n_img_set, n_args = img_info_df.shape
+    for row_i in range(n_img_set):
         # parse arguments
         img_info = img_info_df.iloc[row_i, :]
         required_info = img_info[:4]  # 4 cols expected in doc
         filter_info = img_info[4:].values.astype(str)
         (img_set_path,
             model_path,
             output_path,
-            img_set_name) = _apply_models_check_required_info(required_info)
+            img_set_name) = check_required_info(required_info)
         filter_info = _parse_filter_info(filter_info)
-        # apply model
-        apply_model(img_set_path,
-                    model_path,
-                    output_path,
-                    img_set_name,
-                    filter_info,
-                    write_csv=write_csv,
-                    savefig=savefig)
+        # transform data
+        transform_dataset(
+            img_set_path,
+            model_path,
+            output_path,
+            img_set_name,
+            filter_info,
+            write_csv=write_csv,
+            savefig=savefig
+        )
     return
```

## vampire/util.py

```diff
@@ -82,33 +82,42 @@
 
 def get_properties_csv_path(filepath, filter_info):
     return generate_file_paths(filepath, 'raw-properties', filter_info, '.csv')
 
 
 def get_model_pickle_path(filepath, filter_info, model):
     model_name = model.model_name
-    num_points = model.num_points
-    num_clusters = model.num_clusters
-    num_pc = model.num_pc
-    return generate_file_paths(filepath, f'model_{model_name}_({num_points}_{num_clusters}_{num_pc})',
-                               filter_info,
-                               '.pickle')
+    n_points = model.n_points
+    n_clusters = model.n_clusters
+    n_pcs = model.n_pcs
+    return generate_file_paths(
+        filepath,
+        f'model_{model_name}_({n_points}_{n_clusters}_{n_pcs})',
+        filter_info,
+        '.pickle'
+    )
 
 
 def get_apply_properties_csv_path(filepath, filter_info, model, img_set_name):
     model_name = model.model_name
-    num_points = model.num_points
-    num_clusters = model.num_clusters
-    num_pc = model.num_pc
-    return generate_file_paths(filepath, f'apply-properties_{model_name}_on_{img_set_name}_({num_points}_{num_clusters}_{num_pc})',
-                               filter_info,
-                               '.csv')
+    n_points = model.n_points
+    n_clusters = model.n_clusters
+    n_pcs = model.n_pcs
+    return generate_file_paths(
+        filepath,
+        f'apply-properties_{model_name}_on_{img_set_name}_({n_points}_{n_clusters}_{n_pcs})',
+        filter_info,
+        '.csv'
+    )
 
 
 def get_apply_properties_pickle_path(filepath, filter_info, model, img_set_name):
     model_name = model.model_name
-    num_points = model.num_points
-    num_clusters = model.num_clusters
-    num_pc = model.num_pc
-    return generate_file_paths(filepath, f'apply-properties_{model_name}_on_{img_set_name}_({num_points}_{num_clusters}_{num_pc})',
-                               filter_info,
-                               '.pickle')
+    n_points = model.n_points
+    n_clusters = model.n_clusters
+    n_pcs = model.n_pcs
+    return generate_file_paths(
+        filepath,
+        f'apply-properties_{model_name}_on_{img_set_name}_({n_points}_{n_clusters}_{n_pcs})',
+        filter_info,
+        '.pickle'
+    )
```

## vampire/tests/test_analysis.py

```diff
@@ -95,27 +95,27 @@
 def test_get_cum_explained_variance_ratio(explained_variance_ratio, cum_explained_variance_ratio):
     actual = analysis.get_cum_explained_variance_ratio(explained_variance_ratio)
     expected = cum_explained_variance_ratio
     assert_allclose(actual, expected)
 
 
 def test_get_optimal_num_pc(cum_explained_variance_ratio):
-    actual = analysis.get_optimal_num_pc(cum_explained_variance_ratio)
+    actual = analysis.get_optimal_n_pcs(cum_explained_variance_ratio)
     expected = 7
     assert_allclose(actual, expected)
 
-    actual = analysis.get_optimal_num_pc(cum_explained_variance_ratio, 1)
+    actual = analysis.get_optimal_n_pcs(cum_explained_variance_ratio, 1)
     expected = 9
     assert_allclose(actual, expected)
 
-    actual = analysis.get_optimal_num_pc(cum_explained_variance_ratio, 0.9)
+    actual = analysis.get_optimal_n_pcs(cum_explained_variance_ratio, 0.9)
     expected = 5
     assert_allclose(actual, expected)
 
-    actual = analysis.get_optimal_num_pc(cum_explained_variance_ratio, 0)
+    actual = analysis.get_optimal_n_pcs(cum_explained_variance_ratio, 0)
     expected = 1
     assert_allclose(actual, expected)
 
 
 def test_pca_transform_contours(aligned_contours_flat,
                                 mean_aligned_contour_flat,
                                 pca_contour,
```

## vampire/tests/test_model.py

```diff
@@ -30,18 +30,18 @@
     vampire_model = model.Vampire(model_name,
                                   num_points,
                                   num_clusters,
                                   num_pc,
                                   random_state)
     # model hyperparameters
     assert vampire_model.model_name == model_name
-    assert vampire_model.num_points == num_points
-    assert vampire_model.num_coord == num_coord
-    assert vampire_model.num_clusters == num_clusters
-    assert vampire_model.num_pc == num_pc
+    assert vampire_model.n_points == num_points
+    assert vampire_model.n_coords == num_coord
+    assert vampire_model.n_clusters == num_clusters
+    assert vampire_model.n_pcs == num_pc
     assert vampire_model.random_state == random_state
     # contour info
     assert vampire_model.mean_registered_contour is None
     assert vampire_model.mean_aligned_contour is None
     assert vampire_model.contours is None
     # pca analysis info
     assert vampire_model.principal_directions is None
@@ -66,33 +66,33 @@
     num_pc = 20
     random_state = 1
     vampire_model = model.Vampire(model_name,
                                   num_points,
                                   num_clusters,
                                   num_pc,
                                   random_state)
-    vampire_model.build(properties_df)
+    vampire_model.fit(properties_df)
     actual = vampire_model
     expected = built_model
     assert actual == expected
 
 
 def test_Vampire_apply(properties_df, built_model, apply_properties_df):
     vampire_model = built_model
-    actual = vampire_model.apply(properties_df)
+    actual = vampire_model.transform(properties_df)
     expected = apply_properties_df
     assert_frame_equal(actual, expected)
 
 
 def test_Vampire___eq__(properties_df):
     vampire_model_1_1 = model.Vampire('model_name1',
                                       50, 5, 20, 1)
-    vampire_model_1_1.build(properties_df)
+    vampire_model_1_1.fit(properties_df)
     vampire_model_1_2 = model.Vampire('model_name1',
                                       50, 5, 20, 1)
-    vampire_model_1_2.build(properties_df)
+    vampire_model_1_2.fit(properties_df)
     vampire_model_2 = model.Vampire('model_name2',
                                     50, 5, 20, 2)
-    vampire_model_2.build(properties_df)
+    vampire_model_2.fit(properties_df)
     assert vampire_model_1_1 == vampire_model_1_2
     assert vampire_model_1_1 != vampire_model_2
     assert vampire_model_1_2 != vampire_model_2
```

## vampire/tests/test_plot.py

```diff
@@ -24,17 +24,17 @@
     from vampire import extraction, model, plot
     # properties_df = extraction.extract_properties(
     #     r'C:\Files\github-projects\nance-lab-public\vampire_open\Supplementary Data\Example segmented images\MEF_LMNA--',
     #     np.array(['tiff']))
     properties_df = extraction.extract_properties(
         r'C:\Files\github-projects\nance-lab-data\microfiber\ogd_severity_segmentations',
         np.array(['cortex', 'otsu']))
-    vampire_model = model.Vampire('test-restructure', num_clusters=5, random_state=1)
-    vampire_model.build(properties_df)
-    apply_properties_df = vampire_model.apply(properties_df)
+    vampire_model = model.Vampire('test-restructure', n_clusters=5, random_state=1)
+    vampire_model.fit(properties_df)
+    apply_properties_df = vampire_model.transform(properties_df)
     plot.set_plot_style()
     plot.plot_representatives(vampire_model, apply_properties_df, alpha=0.5)
     plt.show()
     fig, axs = plot.plot_distribution_contour_dendrogram(vampire_model,
                                               apply_properties_df,
                                               height_ratio=[4, 1, 1])
     fig.show()
@@ -57,19 +57,19 @@
     from vampire import quickstart
     img_info_df = pd.DataFrame({
         'img_set_path': [r'C:\Files\github-projects\nance-lab-public\vampire_open\Supplementary Data\Example segmented images\MEF_LMNA--',
                          r'C:\Files\github-projects\nance-lab-public\vampire_open\Supplementary Data\Example segmented images\MEF_wildtype'],
         'output_path': [r'C:\Files\github-projects\nance-lab-public\vampire_open\Supplementary Data\Example segmented images\MEF_LMNA--',
                          r'C:\Files\github-projects\nance-lab-public\vampire_open\Supplementary Data\Example segmented images\MEF_wildtype'],
         'model_name': ['neg', 'wildtype'],
-        'num_points': [np.nan, np.nan],
-        'num_clusters': [np.nan, np.nan],
+        'n_points': [np.nan, np.nan],
+        'n_clusters': [np.nan, np.nan],
         'extension': ['tiff', 'tiff']
     })
-    quickstart.build_models(img_info_df, random_state=1)
+    quickstart.fit_models(img_info_df, random_state=1)
     return
 
 
 def tst_apply_model():
     import pandas as pd
     from vampire import quickstart
     img_info_df = pd.DataFrame({
@@ -78,30 +78,30 @@
         'model_path': [r'C:\Files\github-projects\nance-lab-public\vampire_open\Supplementary Data\Example segmented images\MEF_LMNA--\model_neg__tiff.pickle',
                        r'C:\Files\github-projects\nance-lab-public\vampire_open\Supplementary Data\Example segmented images\MEF_wildtype\model_wildtype__.pickle'],
         'output_path': [r'C:\Files\github-projects\nance-lab-public\vampire_open\Supplementary Data\Example segmented images\MEF_LMNA--',
                          r'C:\Files\github-projects\nance-lab-public\vampire_open\Supplementary Data\Example segmented images\MEF_wildtype'],
         'img_set_name': ['neg', 'wildtype'],
         'extension': ['tiff', 'tiff']
     })
-    quickstart.apply_models(img_info_df)
+    quickstart.transform_datasets(img_info_df)
     return
 
 
 def tst_time():
     import pandas as pd
     from vampire import quickstart
     import time
 
     # tic = time.time()
     # img_info_df = pd.DataFrame({
     #     'img_set_path': [r'C:\Files\github-projects\nance-lab-data\microfiber\ogd_severity_segmentations'],
     #     'output_path': [r'C:\Files\github-projects\nance-lab-data\microfiber\restructure-data'],
     #     'model_name': ['vampire-analysis'],
-    #     'num_points': [np.nan],
-    #     'num_clusters': [np.nan],
+    #     'n_points': [np.nan],
+    #     'n_clusters': [np.nan],
     #     'extension': ['tif']
     # })
     # quickstart.build_models(img_info_df, random_state=1)
     # toc = time.time()
     # build_time = toc - tic
     # print(build_time)
 
@@ -112,15 +112,15 @@
         'model_path': [
             r'C:\Files\github-projects\nance-lab-data\microfiber\restructure-data\model_vampire-analysis__tif.pickle'],
         'output_path': [
             r'C:\Files\github-projects\nance-lab-data\microfiber\restructure-data'],
         'img_set_name': ['all'],
         'extension': ['tif']
     })
-    quickstart.apply_models(img_info_df)
+    quickstart.transform_datasets(img_info_df)
     toc = time.time()
     build_time = toc - tic
     print(build_time)
 
 
 def tst_contour_extraction():
     import numpy as np
@@ -166,19 +166,19 @@
     from vampire import quickstart
     img_info_df = pd.DataFrame({
         'img_set_path': [r'C:\Files\github-projects\nance-lab-data\microfiber\ogd_severity_segmentations',
                          r'C:\Files\github-projects\nance-lab-data\microfiber\ogd_severity_segmentations'],
         'output_path': [r'C:\Files\github-projects\nance-lab-data\microfiber\result-2022-03-10',
                         r'C:\Files\github-projects\nance-lab-data\microfiber\result-2022-03-10'],
         'model_name': ['otsu-all', 'yen-all'],
-        'num_points': [np.nan, np.nan],
-        'num_clusters': [np.nan, np.nan],
+        'n_points': [np.nan, np.nan],
+        'n_clusters': [np.nan, np.nan],
         'threshold': ['otsu', 'yen'],
     })
-    quickstart.build_models(img_info_df, random_state=1)
+    quickstart.fit_models(img_info_df, random_state=1)
     return
 
 
 def tst_apply_model():
     import numpy as np
     import pandas as pd
     from vampire import quickstart
@@ -188,15 +188,15 @@
         'model_path': [r'C:\Files\github-projects\nance-lab-data\microfiber\result-2022-03-10\model_otsu-all__otsu.pickle',
                        r'C:\Files\github-projects\nance-lab-data\microfiber\result-2022-03-10\model_yen-all__yen.pickle'],
         'output_path': [r'C:\Files\github-projects\nance-lab-data\microfiber\result-2022-03-10',
                         r'C:\Files\github-projects\nance-lab-data\microfiber\result-2022-03-10'],
         'img_set_name': ['otsu-all', 'yen-all'],
         'threshold': ['otsu', 'yen'],
     })
-    quickstart.apply_models(img_info_df)
+    quickstart.transform_datasets(img_info_df)
 
 
 def tst_process_data():
     import numpy as np
     import pandas as pd
     from vampire import quickstart, util
     import re
@@ -222,16 +222,16 @@
                 return treatment_names[i]
 
     def label_region(row):
         for i in range(len(region_names)):
             if re.search(region_names[i], row['filename']):
                 return region_names[i]
 
-    otsu_property_df['treatment'] = otsu_property_df.apply(label_treatment, axis=1)
-    otsu_property_df['region'] = otsu_property_df.apply(label_region, axis=1)
+    otsu_property_df['treatment'] = otsu_property_df.transform(label_treatment, axis=1)
+    otsu_property_df['region'] = otsu_property_df.transform(label_region, axis=1)
 
     num_clusters = 5
     # all properties by cluster, all treatment all regions
     for i in range(num_clusters):
         otsu_property_df[(otsu_property_df['cluster_id'] == i)]\
             .drop(['raw_contour', 'normalized_contour'], axis=1)\
             .to_csv(rf'C:\Files\github-projects\nance-lab-data\microfiber\result-2022-03-10\otsu\properties_cluster_{i}_all_treatment_all_regions_all_properties.csv', index=False)
@@ -277,26 +277,26 @@
                                                index=['treatment'],
                                                columns=['cluster_id'])
         cluster_treatment_pivot.to_csv(rf'C:\Files\github-projects\nance-lab-data\microfiber\result-2022-03-10\otsu\cluster-vs-{property}-pivot_all_regions_.csv')
 
     pass
 
 
-def tst_coloring():
-    import numpy as np
-    from vampire import quickstart, util, extraction, coloring
-    filenames = extraction.get_filtered_filenames(r'C:\Files\github-projects\nance-lab-data\microfiber\ogd_severity_segmentations',
-                                                  np.array(['otsu']))
-    img_set = extraction.get_img_set(r'C:\Files\github-projects\nance-lab-data\microfiber\ogd_severity_segmentations', filenames)
-    otsu_property_df = util.read_pickle(
-        r'C:\Files\github-projects\nance-lab-data\microfiber\result-2022-03-10\otsu\apply-properties_otsu-all_on_otsu-all__otsu.pickle')
-    labeled_imgs = coloring.label_imgs(img_set, otsu_property_df)
-    for i, labeled_img in enumerate(labeled_imgs):
-        fig, ax, colors = coloring.color_img(labeled_img)
-        fig.savefig(rf'C:\Files\github-projects\nance-lab-data\microfiber\result-2022-03-10\colored_imgs\colored_{filenames[i]}.png')
+# def tst_coloring():
+#     import numpy as np
+#     from vampire import quickstart, util, extraction, coloring
+#     filenames = extraction.get_filtered_filenames(r'C:\Files\github-projects\nance-lab-data\microfiber\ogd_severity_segmentations',
+#                                                   np.array(['otsu']))
+#     img_set = extraction.get_img_set(r'C:\Files\github-projects\nance-lab-data\microfiber\ogd_severity_segmentations', filenames)
+#     otsu_property_df = util.read_pickle(
+#         r'C:\Files\github-projects\nance-lab-data\microfiber\result-2022-03-10\otsu\apply-properties_otsu-all_on_otsu-all__otsu.pickle')
+#     labeled_imgs = coloring.label_imgs(img_set, otsu_property_df)
+#     for i, labeled_img in enumerate(labeled_imgs):
+#         fig, ax, colors = coloring.color_img(labeled_img)
+#         fig.savefig(rf'C:\Files\github-projects\nance-lab-data\microfiber\result-2022-03-10\colored_imgs\colored_{filenames[i]}.png')
 
 
 def tst_imshow():
     import numpy as np
     from vampire import quickstart, util, extraction, coloring, plot
     import matplotlib.pyplot as plt
     from skimage import io
@@ -323,31 +323,31 @@
     import numpy as np
     import pandas as pd
     from vampire import quickstart
     img_info_df = pd.DataFrame({
         'img_set_path': [r'C:\Files\github-projects\nance-lab-data\ferret-microglia\segmented'],
         'output_path': [r'C:\Files\github-projects\nance-lab-data\ferret-microglia\vampire-crosscheck\output-new'],
         'model_name': ['ferret'],
-        'num_points': [np.nan],
-        'num_clusters': [np.nan],
+        'n_points': [np.nan],
+        'n_clusters': [np.nan],
     })
-    quickstart.build_models(img_info_df, random_state=1)
+    quickstart.fit_models(img_info_df, random_state=1)
     return
 
 
 def tst_apply_model_ferret():
     import numpy as np
     import pandas as pd
     from vampire import quickstart
     img_info_df = pd.DataFrame({
         'img_set_path': [r'C:\Files\github-projects\nance-lab-data\ferret-microglia\segmented'],
         'model_path': [r'C:\Files\github-projects\nance-lab-data\ferret-microglia\vampire-crosscheck\output-new\model_ferret__.pickle'],
         'output_path': [r'C:\Files\github-projects\nance-lab-data\ferret-microglia\vampire-crosscheck\output-new'],
         'img_set_name': ['ferret'],
     })
-    quickstart.apply_models(img_info_df)
+    quickstart.transform_datasets(img_info_df)
 
 
 def tst_model():
     import vampire as vp
     model = vp.util.read_pickle(r'C:\Files\github-projects\nance-lab-data\ferret-microglia\vampire-crosscheck\output-new\model_ferret__.pickle')
     pass
```

## vampire/tests/test_quickstart.py

```diff
@@ -73,17 +73,17 @@
 
 @pytest.fixture
 def build_img_info_df(img_set_path, output_path, model_name,
                       num_points, num_clusters, num_pc):
     img_info = {'img_set_path': img_set_path,
                 'output_path': output_path,
                 'model_name': model_name,
-                'num_points': num_points,
-                'num_clusters': num_clusters,
-                'num_pc': num_pc,
+                'n_points': num_points,
+                'n_clusters': num_clusters,
+                'n_pcs': num_pc,
                 'filename': 'img'}
     img_info_df = pd.DataFrame(img_info, index=np.arange(1))
     return img_info_df
 
 
 @pytest.fixture
 def build_required_info(build_img_info_df):
@@ -120,48 +120,48 @@
 @pytest.fixture
 def apply_model_df():
     return read_abs_pickle(r'data/quickstart/apply_model.pickle')
 
 
 def test__check_prohibited_char():
     # filepath
-    quickstart._check_prohibited_char(r'normal path\very_normal')
-    quickstart._check_prohibited_char(r'path-with/&intere^sting/(charac`ters)/but%/legal~!')
-    quickstart._check_prohibited_char(r'totaly-legal=/+[why]not?!/#@.:;/')
+    quickstart._check_char(r'normal path\very_normal')
+    quickstart._check_char(r'path-with/&intere^sting/(charac`ters)/but%/legal~!')
+    quickstart._check_char(r'totaly-legal=/+[why]not?!/#@.:;/')
     with pytest.raises(ValueError):
-        quickstart._check_prohibited_char(r'paths,not/good')
+        quickstart._check_char(r'paths,not/good')
     with pytest.raises(ValueError):
-        quickstart._check_prohibited_char(r'bad*paths/BAD')
+        quickstart._check_char(r'bad*paths/BAD')
     with pytest.raises(ValueError):
-        quickstart._check_prohibited_char(r'not"allowed/path')
+        quickstart._check_char(r'not"allowed/path')
     with pytest.raises(ValueError):
-        quickstart._check_prohibited_char(r'strictly<not\allowed')
+        quickstart._check_char(r'strictly<not\allowed')
     with pytest.raises(ValueError):
-        quickstart._check_prohibited_char(r'forbiddened|\path')
+        quickstart._check_char(r'forbiddened|\path')
 
     # filename
-    quickstart._check_prohibited_char(r'legal-file_name=with(some)characters.txt', input_type='file')
-    quickstart._check_prohibited_char(r'ToTaLLY!Ok;file`name[it]looks{like}.tiff', input_type='file')
-    quickstart._check_prohibited_char(r'wierd~symb0ls?wel!comed&x^10x2.pdf', input_type='file')
+    quickstart._check_char(r'legal-file_name=with(some)characters.txt', input_type='file')
+    quickstart._check_char(r'ToTaLLY!Ok;file`name[it]looks{like}.tiff', input_type='file')
+    quickstart._check_char(r'wierd~symb0ls?wel!comed&x^10x2.pdf', input_type='file')
     with pytest.raises(ValueError):
-        quickstart._check_prohibited_char(r'not\\good_filename', input_type='file')
+        quickstart._check_char(r'not\\good_filename', input_type='file')
     with pytest.raises(ValueError):
-        quickstart._check_prohibited_char(r'bad/filename', input_type='file')
+        quickstart._check_char(r'bad/filename', input_type='file')
     with pytest.raises(ValueError):
-        quickstart._check_prohibited_char(r'not_allowed,file', input_type='file')
+        quickstart._check_char(r'not_allowed,file', input_type='file')
     with pytest.raises(ValueError):
-        quickstart._check_prohibited_char(r'prohibited:filename', input_type='file')
+        quickstart._check_char(r'prohibited:filename', input_type='file')
     with pytest.raises(ValueError):
-        quickstart._check_prohibited_char(r'strictly*prohibited.txt', input_type='file')
+        quickstart._check_char(r'strictly*prohibited.txt', input_type='file')
     with pytest.raises(ValueError):
-        quickstart._check_prohibited_char(r'no"please', input_type='file')
+        quickstart._check_char(r'no"please', input_type='file')
     with pytest.raises(ValueError):
-        quickstart._check_prohibited_char(r'<worst-filename>', input_type='file')
+        quickstart._check_char(r'<worst-filename>', input_type='file')
     with pytest.raises(ValueError):
-        quickstart._check_prohibited_char(r'don\'t|use', input_type='file')
+        quickstart._check_char(r'don\'t|use', input_type='file')
 
 
 def test__parse_filter_info(empty_filter,
                             img1_filter, img2_filter,
                             img1_nan_filter, img2_nan_filter):
     # empty filter
     actual = quickstart._parse_filter_info(np.array([]))
@@ -184,109 +184,109 @@
 
     actual = quickstart._parse_filter_info(img2_nan_filter)
     expected = img2_filter
     assert_array_equal(actual, expected)
 
 
 def test__build_models_check_df(build_img_info_df):
-    quickstart._build_models_check_df(build_img_info_df)
-    quickstart._build_models_check_df(build_img_info_df.drop('filename', axis=1))
+    quickstart._fit_models_check_df(build_img_info_df)
+    quickstart._fit_models_check_df(build_img_info_df.drop('filename', axis=1))
     with pytest.raises(ValueError):
-        quickstart._build_models_check_df(build_img_info_df.drop(['filename', 'num_clusters'], axis=1))
+        quickstart._fit_models_check_df(build_img_info_df.drop(['filename', 'n_clusters'], axis=1))
 
 
 def test__build_models_check_required_info(build_required_info, img_set_path,
                                            output_path, model_name,
                                            num_points, num_clusters, num_pc):
-    actual = quickstart._build_models_check_required_info(build_required_info)
+    actual = quickstart._fit_models_check_required_info(build_required_info)
     expected = (os.path.normpath(img_set_path),
                 os.path.normpath(output_path),
                 model_name, num_points, num_clusters, num_pc)
     assert_list_equal(actual, expected)
 
     required_info = pd.Series([img_set_path, None, None, None, None, None])
-    actual = quickstart._build_models_check_required_info(required_info)
+    actual = quickstart._fit_models_check_required_info(required_info)
     expected = (os.path.normpath(img_set_path),
                 os.path.normpath(img_set_path),
                 datetime.now().strftime('%Y-%m-%d_%H-%M-%S'),
                 50, 5, None)
     assert_list_equal(actual, expected)
 
 
 def test_build_model(img_set_path, output_path, model_name,
                      num_points, num_clusters, num_pc,
                      empty_filter, random_state,
                      built_model):
-    actual = quickstart.build_model(img_set_path,
-                                    output_path,
-                                    model_name,
-                                    num_points,
-                                    num_clusters,
-                                    num_pc,
-                                    empty_filter,
-                                    random_state=random_state,
-                                    savefig=False)
+    actual = quickstart.fit_model(img_set_path,
+                                  output_path,
+                                  model_name,
+                                  num_points,
+                                  num_clusters,
+                                  num_pc,
+                                  empty_filter,
+                                  random_state=random_state,
+                                  savefig=False)
     actual_write = read_abs_pickle(r'data/quickstart/output/model_quickstart-test_(70_10_25)__.pickle')
     expected = built_model
     assert actual == actual_write
     assert actual == expected
     assert actual_write == expected
 
 
 def test_build_models(build_img_info_df, random_state,
                       built_model):
-    quickstart.build_models(build_img_info_df,
-                            random_state=random_state,
-                            savefig=False)
+    quickstart.fit_models(build_img_info_df,
+                          random_state=random_state,
+                          savefig=False)
     actual_write = read_abs_pickle(r'data/quickstart/output/model_quickstart-test_(70_10_25)__img.pickle')
     expected = built_model
     assert actual_write == expected
 
 
 def test__apply_models_check_df(apply_img_info_df):
-    quickstart._apply_models_check_df(apply_img_info_df)
-    quickstart._apply_models_check_df(apply_img_info_df.drop('filename', axis=1))
+    quickstart._transform_datasets_check_df(apply_img_info_df)
+    quickstart._transform_datasets_check_df(apply_img_info_df.drop('filename', axis=1))
     with pytest.raises(ValueError):
-        quickstart._apply_models_check_df(apply_img_info_df.drop(['filename', 'img_set_name'], axis=1))
+        quickstart._transform_datasets_check_df(apply_img_info_df.drop(['filename', 'img_set_name'], axis=1))
 
 
 def test__apply_models_check_required_info(apply_required_info, img_set_path,
                                            model_path, output_path, model_name):
-    actual = quickstart._apply_models_check_required_info(apply_required_info)
+    actual = quickstart._transform_datasets_check_required_info(apply_required_info)
     expected = (os.path.normpath(img_set_path),
                 os.path.normpath(model_path),
                 os.path.normpath(output_path),
                 model_name)
     assert_list_equal(actual, expected)
 
     required_info = pd.Series([img_set_path, model_path, None, None])
-    actual = quickstart._apply_models_check_required_info(required_info)
+    actual = quickstart._transform_datasets_check_required_info(required_info)
     expected = (os.path.normpath(img_set_path),
                 os.path.normpath(model_path),
                 os.path.normpath(img_set_path),
                 datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))
     assert_list_equal(actual, expected)
 
 
 def test_apply_model(img_set_path, model_path, output_path,
                      model_name, empty_filter, apply_model_df):
-    actual = quickstart.apply_model(os.path.normpath(img_set_path),
-                                    os.path.normpath(model_path),
-                                    os.path.normpath(output_path),
-                                    model_name,
-                                    empty_filter,
-                                    write_csv=False,
-                                    savefig=False)
+    actual = quickstart.transform_dataset(os.path.normpath(img_set_path),
+                                          os.path.normpath(model_path),
+                                          os.path.normpath(output_path),
+                                          model_name,
+                                          empty_filter,
+                                          write_csv=False,
+                                          savefig=False)
     actual_write = read_abs_pickle(r'data/quickstart/output/apply-properties_quickstart-test_on_quickstart-test_(70_10_25)__.pickle')
     expected = apply_model_df
     assert_frame_equal(actual, expected)
     assert_frame_equal(actual, actual_write)
     assert_frame_equal(actual_write, expected)
 
 
 def test_apply_models(apply_img_info_df, apply_model_df):
-    quickstart.apply_models(apply_img_info_df,
-                            write_csv=False,
-                            savefig=False)
+    quickstart.transform_datasets(apply_img_info_df,
+                                  write_csv=False,
+                                  savefig=False)
     actual_write = read_abs_pickle(r'data/quickstart/output/apply-properties_quickstart-test_on_quickstart-test_(70_10_25)__img.pickle')
     expected = apply_model_df
     assert_frame_equal(actual_write, expected)
```

## Comparing `vampire_analysis-0.1.0.dev9.dist-info/LICENSE` & `vampire_analysis-0.2.0.dev1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `vampire_analysis-0.1.0.dev9.dist-info/METADATA` & `vampire_analysis-0.2.0.dev1.dist-info/METADATA`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: vampire-analysis
-Version: 0.1.0.dev9
+Version: 0.2.0.dev1
 Summary: VAMPIRE (Visually Aided Morpho-Phenotyping Image Recognition) analysis quantifies and visualizes heterogeneity of cell and nucleus morphology.
 Home-page: https://github.com/tengjuilin/vampire-analysis
 Author: Teng-Jui Lin
 Author-email: lintengjui@outlook.com
 Classifier: Programming Language :: Python :: 3
 Classifier: License :: OSI Approved :: MIT License
 Classifier: Operating System :: OS Independent
@@ -53,16 +53,16 @@
 >>> build_df = pd.read_excel(r'C:\vampire\build.xlsx')
 >>> vp.model.build_models(build_df, random_state=1)
 ```
 
 If you have `apply.xlsx` under `C:\vampire` containing the apply image set information, you can apply the model with
 
 ```python
->>> apply_df = pd.read_excel(r'C:\vampire\apply.xlsx')
->>> vp.model.apply_models(apply_df)
+>> > apply_df = pd.read_excel(r'C:\vampire\apply.xlsx')
+>> > vp.model.transform_datasets(apply_df)
 ```
 
 Flexible options are provided for [building](https://vampire.readthedocs.io/en/latest/user/build_advanced.html) and [applying](https://vampire.readthedocs.io/en/latest/user/apply_advanced.html) models in the advanced section in the documentation.
 
 ## References
 
 [1] Phillip, J.M., Han, KS., Chen, WC. et al. A robust unsupervised machine-learning method to quantify the morphological heterogeneity of cells and nuclei. *Nat Protoc* **16**, 754774 (2021). <https://doi.org/10.1038/s41596-020-00432-x>
```

## Comparing `vampire_analysis-0.1.0.dev9.dist-info/RECORD` & `vampire_analysis-0.2.0.dev1.dist-info/RECORD`

 * *Files 13% similar despite different names*

```diff
@@ -1,27 +1,27 @@
 vampire/__init__.py,sha256=D_TJWLm3jrsgZOcy1o2yvs6pDIdRc420tUBuNImN8os,239
 vampire/amath.py,sha256=xqwdnhs8aYXqHwPKS7CBjZg8g0Khv7PmeiiBIJUSwQU,12406
-vampire/analysis.py,sha256=xD8vfE7swf6WIPWaGCBa3YqXasjBLrXqh_SZ6DXLqBo,10401
-vampire/coloring.py,sha256=WSNgmiFtlEsBUdb2FJaEgzdc-TJjfQrs3iFWpxcsiVc,3730
-vampire/extraction.py,sha256=rPN68to1chUvd_Ipmyi_IjlDt-7gxHBcJBv0larCYAw,9958
-vampire/model.py,sha256=LqpeIdhYY044_wJzUka2ziStMfXfIvDDtD7HcqccJD4,9959
-vampire/pilot_draw.py,sha256=joIMGa2I0DIxH6CqQwS9GttmS-x7biWRmj3WuDEzc3s,10768
-vampire/plot.py,sha256=YoWfM_0wG4NCHJkwAK0XADxs3ikao6n5QFBdYo5EZS4,14901
-vampire/processing.py,sha256=CUKpNH-1Y0rDQwOW6l9fyb7M6MGFGGhSwnouGOTIn0A,13422
-vampire/quickstart.py,sha256=_sFQs9a2vniIgQHwK4FFHxYWTZT0gyE3zbboL_o3JTQ,22286
-vampire/util.py,sha256=xLnkAqhFaaz7Bf8K7szVtfF_8hw5T9JoV6ydzJffZTs,3370
+vampire/analysis.py,sha256=2p8AVm1ulB8DdWq3wdg8kph22w6nvclvvg5vwkabXOY,10286
+vampire/coloring.py,sha256=UIn79tri7YP5GjBBhcL_LWONAvQP0BgeykFrTWP9i8E,3676
+vampire/extraction.py,sha256=EZ8FuxTWnQnjNkdjorI9GDtIyigGHVYmU9pMw0RTMBQ,9990
+vampire/model.py,sha256=WWXEuf2JmwEI8RIqCcpBBCHPxQM7_ubUXba2sAN5LzA,10161
+vampire/pilot_draw.py,sha256=Qoy9AaXkAMOnjPlX1CJPoos2Euq1qpgAFXlMXX3G1DM,10762
+vampire/plot.py,sha256=qH3lx9YhkzcFcMgeT_KvzkUcA-Na5UYbM3UShSrRk1Q,14888
+vampire/processing.py,sha256=feIjm-rpyB_aBLRPhfyrtHTkL9GOEn0EzGmuVYIv5zE,13427
+vampire/quickstart.py,sha256=YckF87aKfwKah3XyfBA5597V9ClYkR69Bo8sHvFzZOU,20472
+vampire/util.py,sha256=trviARdP591F4WXp_j4Pzxv83cijhxawdCpKhlE8qTo,3262
 vampire/tests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 vampire/tests/test_amath.py,sha256=12LRP-AnVmOV42Tg4ly2y73dPHvEymUJX8u7gGHtc_U,3333
-vampire/tests/test_analysis.py,sha256=s_w3zF47XsO8JzgHTVM7mq5kFvt2wImaSNgtyVPPoFw,6974
+vampire/tests/test_analysis.py,sha256=YRIm7kBuxWMrjL3jYljjS-gOWdD5SUdJnAcCFjrRuq8,6970
 vampire/tests/test_coloring.py,sha256=CBXMiQy53kYVAVZsvvCHMeDgpB8pHm3hGUw1OOzg_KU,2181
 vampire/tests/test_extraction.py,sha256=0kAD9qg7HRAPPMVewXxmx1ctmvU9QshoNRI4q6Vbppw,9409
-vampire/tests/test_model.py,sha256=aRzoBRxJutS6dyajwqvQcZDiR5oZxOzaQijfyPpaIRE,3446
-vampire/tests/test_plot.py,sha256=HbtMzYhQl5RRThkD1qj3vkt8rvu1GzLEqim1n-U-Gkc,16471
+vampire/tests/test_model.py,sha256=6Dy1HWX9xLTtdBbmAlqumg0C9zDPtcJPkbzH_b3ldYM,3436
+vampire/tests/test_plot.py,sha256=X-poXW6ZWAzYqBhSXo0Qdu-pkvp4bwjQvSw-hemUNic,16505
 vampire/tests/test_processing.py,sha256=2NHCejEvS8Oyj9uP1KdElI7pfnfJQpck26sOekPB_Ks,4997
-vampire/tests/test_quickstart.py,sha256=t_2I1erdgFsXBSCy49lQPNBiTUay60O0dekUAxiPj_s,10607
+vampire/tests/test_quickstart.py,sha256=Yhp5kCi9gaRT3ZdDVsUR88dXH9it8ocgvDTTXq8KUvg,10447
 vampire/tests/test_util.py,sha256=a2JXKGSViUFSJLGjRmHdDn0sOSmAziVXzGjzyE7u1Xk,12395
 vampire/tests/testing.py,sha256=znZD8BrawAW8mvPIjG6Vj4eW36YJ9R_tsKHaBgk-Af4,663
-vampire_analysis-0.1.0.dev9.dist-info/LICENSE,sha256=gcuuhKKc5-dwvyvHsXjlC9oM6N5gZ6umYbC8ewW1Yvg,35821
-vampire_analysis-0.1.0.dev9.dist-info/METADATA,sha256=_t725RrbvDdsaIzgbTavMqKkH3ixjl9NKlL4LRhCFFM,3876
-vampire_analysis-0.1.0.dev9.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-vampire_analysis-0.1.0.dev9.dist-info/top_level.txt,sha256=T_vFUy60TbYIuWlBxsc2DfO_SokyPIO9Zk6GVzXqjCE,8
-vampire_analysis-0.1.0.dev9.dist-info/RECORD,,
+vampire_analysis-0.2.0.dev1.dist-info/LICENSE,sha256=gcuuhKKc5-dwvyvHsXjlC9oM6N5gZ6umYbC8ewW1Yvg,35821
+vampire_analysis-0.2.0.dev1.dist-info/METADATA,sha256=E9SQ830dd__TH3p4Rel-eoURQCGG-z8KLfRfOmjlL28,3884
+vampire_analysis-0.2.0.dev1.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+vampire_analysis-0.2.0.dev1.dist-info/top_level.txt,sha256=T_vFUy60TbYIuWlBxsc2DfO_SokyPIO9Zk6GVzXqjCE,8
+vampire_analysis-0.2.0.dev1.dist-info/RECORD,,
```

